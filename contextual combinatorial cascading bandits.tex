%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% my settings
\usepackage{amssymb,amsfonts,amsmath,bbm}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage[all]{xy}
\usepackage{empheq}
\usepackage{bm}
\usepackage{algorithm,algorithmic}
\usepackage{multirow}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bOne}{\mathbbm{1}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\trace}{\mathrm{trace}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\| #1 \|}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}%[chapter]
\newtheorem{definition}[theorem]{Definition}%[chapter]
\newtheorem{corollary}[theorem]{Corollary}%[chapter]
\newtheorem{lemma}[theorem]{Lemma}%[chapter]
\newtheorem{example}[theorem]{Example}
\newenvironment{proof}{\noindent {\textbf{Proof. }}}{$\Box$ \medskip}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Contextual Combinatorial Cascading Bandits}

\begin{document} 
	
\twocolumn[
\icmltitle{Contextual Combinatorial Cascading Bandits}
	
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute, 314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute, 27182 Exp St., Toronto, ON M6H 2T1 CANADA}
	
% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}
	
\vskip 0.3in
]
	
\begin{abstract} 
The purpose of this document is to provide both the basic paper template and submission guidelines.
\end{abstract} 
	
\section{Introduction}
	
\section{Related Works}
	
\section{Problem Formulation}
	
We model our problem as a contextual combinatorial cascading bandit. Suppose we have $E=\{1,...,L\}$ a finite set of $L$ ground items. Let $\prod^k=\{(a_1,...,a_k): a_1,...,a_k \in E, a_i \neq a_j \text{ for any } i \neq j\}$ be the set of all $k$-tuples of distinct items from $E$. Let $\cS \subset \prod^{\leq K}(E)$ consist of feasible actions with length no more than $K$.
	
At time $t$, the learning agent is revealed with feature vectors $x_{t,a} \in \RR^{d \times 1}$ for every basic arm $a \in E$, where we have $\norm{x_{t,a}}_2 \leq 1$; this feature vector combines both information of the user and the corresponding basic arm. We use Bernoulli random variable $\bw_{t}(a) \in \{0,1\}$, which is called {\it weight} for arm $a$ at time $t$, to indicate the output of the item $a$ at time $t$. Assume $\bw_{t,a}$ are mutually independent and satisfy
\begin{equation}
\label{eq:expectation}
\EE[\bw_{t,a} | x_{t,a}] = \theta_*^{\top} x_{t,a}
\end{equation}
where $\theta_*$ is an unknown $d$-dimensional vector with the assumption that $\norm{\theta_*}_2 \leq 1$ and $0 \leq \theta_*^{\top} x_{t,a} \leq 1$ for all $t, a$. At time $t$, the learning agent chooses a solution $\bA_t=(\ba_{1}^t,...,\ba_{\abs{\bA_t}}^t) \in \cS$ based on its past observations. At the end of every time $t$, the agent observes weights of the first $\bO_t$ items of $\bA_t$ where 
$$
\bO_t \leq \abs{\bA_t}, \bw_t(\ba_k^t) = 1, \forall k < \bO_t, \bw_t(\ba_{\bO_t}^t) = 0
$$ 
or 
$$
\bO_t = \abs{\bA_t}, \bw_t(\ba_k^t) = 1, \forall k \leq \abs{\bA_t}.
$$
We say that item $a$ is {\it observed} if $a = \ba_k^t$ for some $k \leq \bO_t$. 

The agent receives reward $1$ if all items in $\bA_t$ are observed and have weights $1$. Suppose we consider partial reward, $0 \leq \gamma_1 \leq \ldots \leq \gamma_{K-1} \leq 1$, for partial number of observed items with weight $1$: If $\bO_t = 1 < \abs{\bA_t}$, then the agent only observes the weight of the first item of $\bw_t(\ba_1^t)$ and receives zero reward; if $1 < \bO_t < \abs{\bA_t}$, then the agent observes weights of first $\bO_t$ items among which $\bO_t - 1$ items have weights 1 and receives reward $\gamma_{\bO_t - 1}$; if $\bO_t = \abs{\bA_t}$ and $\bw_{\ba_{\abs{\bA_t}}^t} = 0$, then the agent observes $\abs{\bA_t} - 1$ items with weights $1$ and receives reward $\gamma_{\abs{\bA_t} - 1}$. Note that the order of $\bA_t$ affects both the feedback and the reward. In the analysis part, we will show how the order affects the reward.

At the end of time $t$, the agent observes $\bO_t, \bw_t(\ba_k^t), 1\leq k\leq\bO_t$ and receives the reward
\begin{align*}
\br_t &= \begin{cases}
\gamma_{\bO_t - 1}  &\text{if } \bw_t(\ba_{\bO_t}^t) = 0\\
1 &\text{if } \bw_t(\ba_{\bO_t}^t) = 1,
\end{cases}\\
&=\begin{cases}
\gamma_{k - 1}  &\text{if } \bw_t(\ba_{i}^t) = 1, i\leq k-1, \bw_t(\ba_{k}^t) = 0, k\leq \abs{\bA_t}\\
1 &\text{if } \bw_t(\ba_{i}^t) = 1, i\leq \abs{\bA_t},
\end{cases}
\end{align*}
where we use the notation of $\gamma_0 = 0$.
	
Given partial reward parameters $0 \leq \gamma_1 \leq \ldots \leq \gamma_{K-1} \leq 1$, we introduce a function $f$ on $A=(a_1,...,a_{\abs{A}}) \in \cS, w=(w(1),...,w(L))$ by
\begin{align*}
&f : \cS \times [0,1]^E \to [0,1]\\
&f(A,w) = \sum_{k=2}^{\abs{A}}\gamma_{k-1} (\prod_{i=1}^{k-1}w(a_i))(1 - w(a_k)) + \prod_{i=1}^{\abs{A}}w(a_i),
\end{align*}
then we have $\br_t = f(\bA_t, \bw_t)$ and $\EE[\br_t]=f(\bA_t,\theta_*^{\top}x_t)$ where $x_t=(x_{t,1}, \cdots, x_{t,L}) \in \RR^{d \times L}$. Let 
\begin{align*}
A_t^* &= \argmax_{A\in \cS} f(A,\theta_*^{\top}x_t),\\
f_t^* &= f(A_t^*, \theta_*^{\top}x_t).
\end{align*}
The goal of our learning algorithm is to minimize the expected cumulative regret in $T$ steps
$$
R(T) = \EE[\sum_{t=1}^T \Delta_{t,\bA_t}].
$$
where $\Delta_{t,\bA_t} = f_t^* - f(\bA_t, \theta_*^{\top}x_t)$.



\section{Algorithms and Results}

\subsection{Algorithm}
	
Let $\cH_t$ denote all history at the end of time $t$; $\cH_t$ consists of $\{x_s, \bA_s=(\ba_{1}^s,...,\ba_{\abs{\bA_s}}^s), \bO_s, \bw_s(\ba_k^s), k \in[\bO_s], s\in[t] \}$ and define $\EE_t[\cdot] = \EE[\cdot | \cH_t]$. Here we abuse the notation $\abs{\bA_s}$ to denote the number of elements in the list $\bA_s$. Let
$$
\gamma_k' = 1 - \gamma_{k-1}, ~~~1 \leq k \leq K.
$$
By equation (\ref{eq:expectation}), we have 
$$
\EE[(\gamma_k'\bw_{s,\ba_k^s}) | \cH_{s-1}] = \theta_*^{\top} (\gamma_k' x_{s,\ba_k^s}).
$$
By ridge regression of data 
$$
\{(\gamma_k' X_{s,\ba_k^s}, \gamma_k'\bw_{s,\ba_k^s})\}_{k \in[\bO_s], s\in[t]},
$$
we get a $l^2$-regularized least-squares estimate of $\theta_*$ with regularization parameter $\lambda > 0$:
\begin{equation}
\hat{\theta}_t = (\bX_t^{\top}\bX_t + \lambda I)^{-1} \bX_t^{\top} \bY_t,
\end{equation}
where $\bX_t \in \RR^{(\sum_{s=1}^{t}\bO_s) \times d}$ is the matrix whose rows are $\gamma_k' x_{s,\ba_k^s}^{\top}$ and $\bY_t$ is the column vector whose elements are $\gamma_k' \bw_s(\ba_k^s)$, $k \in[\bO_s], s\in[t]$. Let
$$
\bV_t = \bX_t^{\top} \bX_t + \lambda I = \lambda I + \sum_{s=1}^{t} \sum_{k=1}^{\bO_s} \gamma_k'^2 x_{s,\ba_k^s}x_{s,\ba_k^s}^{\top}.
$$
Then $\bV_t \in \RR^{d \times d}$ is a positive invertible matrix.

Next we build on a good estimate of differences between $\hat{\theta}_t$ and $\theta_*$ by Theorem 2 in \cite{abbasi2011improved}, which states the following results.
	
\begin{theorem}[\cite{abbasi2011improved}]
\label{thm:theta_estimate}
Let 
$$
\beta_{t}(\delta) = \sqrt{\log(\det(\bV_{t})) + 2 \log(\frac{1}{\delta})} + \sqrt{\lambda}.
$$
Then for any $\delta > 0$, with probability at least $1 - \delta$, for all $t > 0$, we have
\begin{equation}
\label{eq:estimateTheta}
\norm{\hat{\theta}_t - \theta_*}_{\bV_{t}} \leq \beta_{t}(\delta).
\end{equation}
\end{theorem}

This theorem states that with high probability, the estimate $\hat{\theta}$ lies in the ellipsoid centered at $\theta_*$  with confidence radius $\beta_t(\delta)$ under $\bV_t$ norm. Building on this, we can define an upper confidence bound of the expected weight of item $a$ by
\begin{equation}
\bU_t(a) = \min\{\hat{\theta}_{t-1}^{\top}x_{t,a} + \beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}, 1\}.
\end{equation}

The fact that $\bU_t(a)$ is an upper confidence bound of $\theta_*^{\top}x_{t,a}$ is proved in the following lemma.
\begin{lemma}
\label{lem:estimateU}
When (\ref{eq:estimateTheta}) holds for time $t$, we have
$$
0 \leq \bU_t(a) - \theta_*^{\top}x_{t,a} \leq 2\beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}.
$$
\end{lemma}
\begin{proof}
\begin{align*}
\abs{\hat{\theta}_{t-1}^{\top}x_{t,a} - \theta_*^{\top}x_{t,a}} &\leq \norm{\hat{\theta}_{t-1} - \theta_*}_{\bV_{t-1}} \norm{x_{t,a}}_{\bV_{t-1}^{-1}} \\
&\leq \beta_{t}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}.
\end{align*}
\end{proof}

Our proposed algorithm, ConComCascade, is described in Algorithm \ref{alg:ConComCascade}. First, it computes the upper confidence bounds (UCBs) $\bU_t \in [0,1]^{E}$ on the expected weights of all items in $E$. Second, it recommends the output $\bA_t$ by the offline oracle with estimated weights $\bU_t$. Third, play $\bA_t$ and observes all feedback of items until first $0$ weight come out; the learning agent observes $\bO_t$ items and $\bw_t(\ba_k^t), 1 \leq k \leq \bO_t$, where $\bw_t(\ba_{\bO_t}^t) = 0, \bw_t(\ba_k^t) = 1, 1 \leq k < \bO_t$ or $\bw_t(\ba_k^t) = 1, 1 \leq k \leq \bO_t = \abs{\bA_t}$. Then, the learning agent update $\bV_t, \bX_t, \bY_t$ in order to get a newer estimate $\hat{\theta}_t$ of $\theta_*$ and new confidence radius $\beta_t(\delta)$. In this part, we use the notation $[A; B]$ to denote a matrix $\begin{pmatrix} A\\ B\end{pmatrix}$.

\begin{algorithm}
\caption{ConComCascade}
\label{alg:ConComCascade}
\begin{algorithmic}[1]
\STATE {//Initialization}
\STATE {Parameters: $\delta > 0, \lambda >0, 0 = \gamma_0 \leq \gamma_1 \leq \cdots \leq \gamma_{K-1} \leq 1$}
\STATE {$\hat{\theta}_0 = 0, \beta_0(\delta) = 1, \bV_0 = \lambda I$}
\STATE{}

\FORALL {$t=1,2,\ldots$}
\STATE {Obtain context $x_{t,a}$ for all $a\in E$}
\STATE{}

\STATE {//Compute UCBs}
\STATE {$\forall a\in E: $}
\STATE {$\bU_t(a) \leftarrow \min\{\hat{\theta}_{t-1}^{\top}x_{t,a} + \beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}, 1\}$}
\STATE {}
		
\STATE {//Choose action $\bA_t$ using UCBs $\bU_t$}
\STATE {$\bA_t=(\ba_1^t,...,\ba_{\abs{\bA_t}}^t) \leftarrow \argmax_{A \in \cS} f(A, \bU_t)$}
\STATE {Play $\bA_t$ and observe $\bO_t, \bw_t(\ba_k^t), k\in[\bO_t]$}
\STATE {}

\STATE {//Update statistics}
\STATE {$\bV_{t} \leftarrow \bV_{t-1} + \sum_{k=1}^{\bO_t} \gamma_k^2 x_{t, \ba_k^t}x_{t, \ba_k^t}^{\top}$}
\STATE {$\bX_t \leftarrow [\bX_{t-1}^\top; ~\gamma_1' x_{t, \ba_1^t}^{\top};  ~\ldots; ~\gamma_{\bO_t}' x_{t, \ba_{\bO_t}^t}^{\top}]$}
\STATE {$\bY_t \leftarrow [\bY_{t-1}^\top; ~\gamma_1' \bw(\ba_1^t); ~\ldots; ~\gamma_{\bO_t}' \bw_t(\ba_{\bO_t}^t)^{\top}]$}
\STATE {$\hat{\theta}_t \leftarrow (\bX_t^\top \bX_t + \lambda I)^{-1} \bX_{t}^\top \bY_t$}
\STATE {$\beta_{t}(\delta) \leftarrow \sqrt{\log(\det(\bV_{t})) + 2 \log(\frac{1}{\delta})} + \sqrt{\lambda}$}
			
\ENDFOR {~~$t$}
\end{algorithmic}
\end{algorithm}
	

\subsection{Results}

Here is our main theorem.
\begin{theorem}
Given partial reward parameters $0 = \gamma_0 \leq \gamma_1 \leq \cdots \gamma_{K-1} \leq 1$ and regularization parameter $\lambda \geq C_\gamma$, for any $\delta > 0$, with probability at least $1 - \delta$, the cumulative expected regret of our algorithm, {\it ConComCascade}, satisfies
\begin{equation}
R(T) = O(\frac{d}{f^*}\log(C_\gamma T)\sqrt{KT}),
\end{equation}
where
\begin{align*}
f^* &= \min_{1 \leq t \leq T} f_t^*\\
C_\gamma &= \sum_{k=1}^{K} \gamma_k'^2 = \sum_{k=1}^{K-1} (1 - \gamma_k)^2 = O(K).
\end{align*}
\end{theorem}
\begin{proof}
With probability at least $1-\delta$,
\begin{equation}
\begin{split}
&R_T =\sum_{t=1}^{T} \EE_{t}[\Delta_{t, \bA_t}] \\
&\leq \sum_{t=1}^{T} 4 \beta_{t-1}(\delta) \EE[\sum_{k=1}^{\bO_t}\norm{\gamma_k' X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}]\\
&\leq 4 \beta_T(\delta) \sum_{t=1}^{T} \bO_t \EE[\sum_{k=1}^{\bO_t}\norm{\gamma_k'^2 X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]\\
&\leq 4 \beta_T(\delta) \sqrt{(\sum_{t=1}^{T} \bO_t) \EE[\sum_{t=1}^{T} \sum_{k=1}^{\bO_t}\norm{\gamma_k'^2 X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]}  \\
&\leq O(d\log(C_\gamma T)\sqrt{TK})
\end{split}
\end{equation}
\end{proof}
	
\begin{lemma}
\label{lem:increasing} 
If $0= \gamma_0 \leq \gamma_1 \leq ... \leq \gamma_{K-1} \leq 1$ and we have $0 \leq w \leq w' \leq 1$ as column vectors in $\RR^L$, then for any $A\in\cS$,
$$
f(A, w) \leq f(A, w').
$$
\end{lemma}
\begin{proof}
It suffices to prove that $\cA = [m]$ where $1 \leq m \leq K$. Because for $1 \leq k \leq m$,
\begin{align*}
&~~-\gamma_{k-1} + \sum_{i=k+1}^m \gamma_{i-1}(\prod_{j=k+1}^{i-1}w_j')(1- w_i') + \prod_{j=k+1}^{m}w_j'\\
&\geq \gamma_{k-1} (-1 + \sum_{i=k+1}^m (\prod_{j=k+1}^{i-1}w_j')(1- w_i') +  \prod_{j=k+1}^{m}w_j')\\
&=\gamma_{k-1} \cdot 0 = 0,
\end{align*}
then
\begin{align*}
&~~\gamma_{k-1} (1 - w_k) + w_k\sum_{i=k+1}^m \gamma_{i-1}(\prod_{j=k+1}^{i-1}w_j')(1- w_i')\\
&\qquad \qquad + w_k\prod_{j=k+1}^{m}w_j'\\
& \leq \gamma_{k-1} (1 - w_k') + w_k'\sum_{i=k+1}^m \gamma_{i-1}(\prod_{j=k+1}^{i-1}w_j')(1- w_i')\\
&\qquad \qquad + w_k'\prod_{j=k+1}^{m}w_j'.
\end{align*}
Therefore, 
\begin{align*}
& f(A; w_1,...,w_{k-1},w_k,w_{k+1}',...,w_m')\\
&=\sum_{i=2}^{k-1} \gamma_{i-1} (\prod_{j=1}^{i-1}w_j)(1 - w_i) + (\prod_{j=1}^{k-1}w_j) [\gamma_{k-1} (1 - w_k) \\
&\qquad + w_k\sum_{i=k+1}^m \gamma_{i-1}(\prod_{j=k+1}^{i-1}w_j')(1- w_i') + w_k\prod_{j=k+1}^{m}w_j']\\
&\leq \sum_{i=2}^{k-1} \gamma_{i-1} (\prod_{j=1}^{i-1}w_j) (1 - w_i) + (\prod_{j=1}^{k-1}w_j) [\gamma_{k-1} (1 - w_k') \\
&\qquad + w_k'\sum_{i=k+1}^m \gamma_{i-1} (\prod_{j=k+1}^{i-1}w_j')(1- w_i') + w_k'\prod_{j=k+1}^{m}w_j']\\
&=f(A; w_1,...,w_{k-1},w_{k}',w_{k+1}',...,w_m').
\end{align*}
Then we obtain the result.
\end{proof}
	
\begin{lemma}
\label{lem:estimateTech}
Suppose $0 \leq p_1,...,p_m \leq 1$ and $u_1,...,u_m \geq 0$. Let $w_i = \min\{p_i + u_i, 1\}, 1 \leq i \leq m$. Then
$$
f(A^{(m)}, w) \leq f(A^{(m)}, p) + \sum_{k=1}^{m} \gamma_k' u_k,
$$
where $A^{(m)} = (1,2,\ldots,m), w = (w_1, \ldots, w_m)$ and $p = (p_1, \ldots, p_m)$.
\end{lemma}
\begin{proof}
We prove this by induction. It holds obviously for $m=1$.
\begin{align*}
&~~f(m+1, w)\\
&= f(m, w) -\prod_{k=1}^{m} w_k + \gamma_{m}  (\prod_{k=1}^{K} w_k) (1 - w_{K+1})+ \prod_{k=1}^{K+1} w_k\\
&= f(m, w) + (\gamma_{m} - 1) (\prod_{k=1}^{m} w_k) (1 - w_{m+1})\\
&\leq f(m, w) + (\gamma_{m} - 1) (\prod_{k=1}^{m} p_k) (1 - w_{m+1})\\
&\leq f(m, w) + (\gamma_{m} - 1) (\prod_{k=1}^{m} p_k)  (1 - p_{m+1} - u_{m+1})\\
&\leq f(m, p) +  \sum_{k=1}^{m} \gamma_k' u_k + (\gamma_{m} - 1) (1 - p_{m+1}) \prod_{k=1}^{m} p_k\\
&~~ + (1 - \gamma_{m}) u_{m+1}\\
&\leq f(m+1, p) + \sum_{k=1}^{m+1} \gamma_k' u_k
\end{align*}
\end{proof}
	
Let $p_A$ denote the probability that all of items in $A$ are observed. Then $p_A = \prod_{k=1}^{\abs{A}-1}w(a_i)$.

\begin{lemma}
Suppose $A = (a_1, ..., a_{\abs{A}})$. Let $B_k = (a_1, ..., a_k), k \leq \abs{A}$ be a prefix of $A$. Then we have the following properties.
\begin{itemize}
\item[(1)] $f(A, w) \leq p_A + \gamma_{\abs{A} - 1}$;
\item[(2)] $f(B_{k+1}, w) \leq f(B_k, w)$;
\item[(3)] $p_{B_{k+1}} \leq p_{B_k}$;
\item[(4)] $f(B_k, w) \leq p_{B_{k+1}} + \gamma_{k}$.
\end{itemize}	
\end{lemma}

\begin{proof}
It suffices to prove these properties when $A = A^{(m)}$ and $w = (w_1, \ldots, w_m)$.
\begin{itemize}
\item[(1)]
$f(A, w) \leq \gamma_{m-1}(1 - p_A w_m) + p_A w_m \leq \gamma_{m-1} + p_A$;

\item[(2)]
\begin{align*}
&f(B_k, w) - f(B_{k+1}, w)\\
&=\prod_{i=1}^{k}w_i - \gamma_{k} (\prod_{i=1}^{k}w_i) (1 - w_{k+1}) - (\prod_{i=1}^{k}w_i) w_{k+1}\\
&=(1 - \gamma_{k}) (\prod_{i=1}^{k}w_i) (1 - w_{k+1}) \geq 0.
\end{align*}

\item[(3)]
Obvious.

\item[(4)]
$$
f(B_k, w) \leq \gamma_{k-1} (1 - p_{B_{k+1}}) + p_{B_{k+1}}\leq p_{B_{k+1}} + \gamma_{k}
$$
\end{itemize}
\end{proof}

\begin{proposition}
Suppose $A = (a_1, ..., a_{\abs{A}})$. Then there exists a prefix $B$ of $A$, such that 
$$
p_{B} \geq \frac{1}{2}f^* - \gamma_{\abs{B}-1}, \qquad \Delta_{B} \geq \frac{1}{2}\Delta_A.
$$ 
\end{proposition}
\begin{proof}
	Notice that $f^* \leq 1$. By above lemma, we have $[f(A,w), 1] \subset \bigcup [f(B_k,w), p_{B_k} + \gamma_{k-1}]$.
\end{proof}


\begin{proposition}
Suppose $\gamma_{K-1} \leq \frac{1}{4} f^*$. With high probability $1-\delta$, 
$$
E[\Delta_{t, \bA_t}|\cH_t] \leq \frac{4}{f^*} \beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k' x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
\end{proposition}
\begin{proof}
By above proposition, there exists a prefix $\bB_t$ of $\bA_t$ such that $p_{\bB_t} \geq \frac{1}{4}f_t^*$ and $\Delta_{\bB_t} \geq \frac{1}{2}\Delta_A$. Then we have
$$
\EE[\Delta_{t, \bA_t} | \cH_{t-1}] \leq \frac{4}{f_t^*} \EE[\Delta_{t, \bB_t} \bOne\{\Delta_{t, \bB_t} > 0, \bO_t \geq \abs{\bB_t}\}
$$
Using $f(\bA_t^*,\bU_t) \leq f(\bA_t,\bU_t) \leq f(\bB_t,\bU_t)$ and lemma \ref{lem:estimateU}, \ref{lem:increasing}, it holds that
$$
f(\bA_t^*, \theta_*^{\top}X_t) \leq f(\bB_t,\bU_t).
$$
Then by lemma \ref{lem:estimateTech}
$$
f(\bB_t,\bU_t) \leq f(\bB_t, \theta_*^{\top}x_t) + \sum_{k=1}^{\abs{\bB_t}}\gamma_k'\beta_{t-1}(\delta)\norm{x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
Therefore,
$$
\Delta_{\bB_t} \leq \sum_{k=1}^{\abs{\bB_t}}\beta_{t-1}(\delta)\norm{\gamma_k' x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
\end{proof}
	
\begin{lemma} % det(V_t)
$\det(\bV_t) \leq (\lambda + C_\gamma t/d)^d.$
\end{lemma}
\begin{proof}
We have $\det(\bV_t) \leq (\trace(\bV_t)/d)^d$ and
\begin{align*}
&\trace(\bV_t)\\
& = \trace(\lambda I) + \sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s,\abs{\bA_s}\}} \gamma_k'^2 \trace(x_{s,\ba_k^s} x_{s,\ba_k^s}^{\top})\\	
& = d \lambda + \sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s,\abs{\bA_s}\}} \gamma_k'^2 \norm{x_{s,\ba_k^s}}_2^2\\
& \leq d \lambda + \sum_{s=1}^t\sum_{k=1}^{K}\gamma_k'^2=d\lambda + tC_\gamma.
\end{align*}
\end{proof}

\begin{lemma} % technical lemma for det
Let $x_i \in \RR^{d \times 1}, 1 \leq i \leq n$. Then we have
$$
\det(I + \sum_{i=1}^n x_i x_i^{\top}) \geq 1 + \sum_{i=1}^n \norm{x_i}_2^2.
$$
\end{lemma}
\begin{proof}
Denote the eigenvalues of $I + \sum_{i=1}^n x_i x_i^{\top}$ by $1+\alpha_1,...,1+\alpha_d$ with $\alpha_j \geq 0$, $1\leq j\leq d$. Then
\begin{align*}
&\det(I + \sum_{i=1}^n x_i x_i^{\top})\\
&= \prod_{j=1}^d (1 + \alpha_j)\geq 1 +\sum_{j=1}^d \alpha_j =1-d + \sum_{i=1}^d (1+\alpha_i) \\
&=1-d + \trace(I + \sum_{i=1}^n x_i x_i^{\top})= 1-d + d + \sum_{i=1}^n \norm{x_i}_2^2\\
&=1 + \sum_{i=1}^n \norm{x_i}_2^2.
\end{align*}
\end{proof}

\begin{lemma}
If $\lambda \geq C_\gamma$, it holds that
$$
\sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s, \abs{\bA_s}\}} \norm{\gamma_k' x_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \leq 2\log(\det(\bV_t)).
$$
\end{lemma}
\begin{proof}
\begin{align*}
&\det(\bV_t) = \det(\bV_{t-1} + \sum_{k=1}^{\bO_t} (\gamma_k' X_{t,\ba_k^{t}})(\gamma_k' X_{t, \ba_k^{t}}^{\top}))\\
&=\det(\bV_{t-1})\\
&~~~~\det(I + \sum_{k=1}^{\bO_t} \gamma_k'\bV_{t-1}^{-1/2}x_{t,\ba_{k}^{t}} (\gamma_k' \bV_{t-1}^{-1/2}x_{t,\ba_{k}^{t}})^{\top})\\
&\geq \det(\bV_{t-1}) (1 + \sum_{k=1}^{\bO_t} \norm{\gamma^{k-1}x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2)\\
&\geq \det(\lambda I)\prod_{s=1}^{t}(1 + \sum_{k=1}^{\bO_s} \norm{\gamma^{k-1}x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)
\end{align*}
Because
$$
\norm{\gamma_k' x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2 \leq \norm{\gamma_k' x_{s,\ba_k^s}}_2^2/\lambda_{\min}(\bV_{s-1}) \leq \gamma_k'^2 /\lambda,
$$
we have 
$$
\sum_{k=1}^{\bO_s} \norm{\gamma_k' x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2 \leq \frac{1}{\lambda} \sum_{k=1}^{K} \gamma_k'^2 = C_\gamma /\lambda \leq 1
$$
Using the fact that $ 2\log(1+u) \geq u$ for any $u \in [0,1]$, we get
\begin{align*}
&\sum_{s=1}^t \sum_{k=1}^{\bO_s}\norm{\gamma_k' x_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \\
&\leq 2\sum_{s=1}^t\log(1 + \sum_{k=1}^{\bO_s} \norm{\gamma_k' x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)\\
&\leq 2(\log(\det(\bV_t)) - \log(\det(\lambda I))) < 2\log(\det(\bV_t))
\end{align*}
\end{proof}
	
	
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}
	
\bibliography{cascade_reference}
\bibliographystyle{icml2015}
	
\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
