%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}
\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = blue, %Colour for external hyperlinks
	linkcolor    = blue, %Colour of internal links
	citecolor   = blue %Colour of citations
}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% my settings
\usepackage{amssymb,amsfonts,amsmath,bbm}
\usepackage{mathrsfs}
\usepackage[all]{xy}
\usepackage{empheq}
\usepackage{bm}
\usepackage{algorithm,algorithmic}
\usepackage{multirow}
\usepackage{ifthen}
\usepackage[usenames]{color}
\usepackage[usenames,dvipsnames]{xcolor}


\newcommand{\EE}{\mathbb{E}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bOne}{\mathbbm{1}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\trace}{\mathrm{trace}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\| #1 \|}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}%[chapter]
\newtheorem{definition}[theorem]{Definition}%[chapter]
\newtheorem{corollary}[theorem]{Corollary}%[chapter]
\newtheorem{lemma}[theorem]{Lemma}%[chapter]
\newtheorem{example}[theorem]{Example}
\newenvironment{proof}{\noindent {\textbf{Proof. }}}{$\Box$ \medskip}

\newcommand{\compilehidecomments}{false}

% Macro for comments:
\ifthenelse{ \equal{\compilehidecomments}{true} }{%
	\newcommand{\wei}[1]{}
	\newcommand{\yang}[1]{}
	\newcommand{\yajun}[1]{}
}{
\newcommand{\wei}[1]{{\color{blue!50!black}  [\text{Wei:} #1]}}
\newcommand{\shuai}[1]{{\color{brown!60!black} [\text{Shuai:} #1]}}
\newcommand{\shengyu}[1]{{\color{green!50!black} [\text{Shengyu:} #1]}}
}


% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Contextual Combinatorial Cascading Bandits}

\begin{document} 
	
\twocolumn[
\icmltitle{Contextual Combinatorial Cascading Bandits}
	
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute, 314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute, 27182 Exp St., Toronto, ON M6H 2T1 CANADA}
	
% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}
	
\vskip 0.3in
]
	
\begin{abstract} 
The purpose of this document is to provide both the basic paper template and submission guidelines.
\end{abstract} 
	
\section{Introduction}
	
\section{Related Works}
	
\section{Problem Formulation}

We formulate the problem of {\em contextual combinatorial cascading bandit} as follows. Suppose we have a finite set of $L$ ground items,  $E=\{1,...,L\}$, also referred to as {\em base arms}. 
Let $\prod^k=\{(a_1,...,a_k): a_1,...,a_k \in E, a_i \neq a_j \text{ for any } i \neq j\}$ be the set of all $k$-tuples of distinct items from $E$. We also refer to each of such tuples as an {\em action}.
Let $\cS \subseteq \cup_{k=1}^K \prod^{k}$ be the set of feasible actions with length no more than $K$. 
As a convention, we always use boldface symbols to represent random variables.

At time $t$, feature vectors $x_{t,a} \in \RR^{d \times 1}$ with $\norm{x_{t,a}}_2 \leq 1$ for every base arm $a \in E$ are revealed to the learning agent; the feature vectors combine both the information of the user and the corresponding base arm. Then the learning agent recommends a feasible action $\bA_t=(\ba_{1}^t,...,\ba_{\abs{\bA_t}}^t) \in \cS$ to the user. 
Starting from the first item, the user checks the items one by one. The checking process stops if the user clicks on one item or has checked all items without clicking anyone. We use Bernoulli random variable $\bw_{t}(a) \in \{0,1\}$ as the {\em weight} of base arm $a$ at time $t$ to indicate whether the user has clicked on $a$ or not. 

Let $\cH_t$ denote the history before the learning agent chooses action at time $t$.
Thus $\cH_t$ contains all information and observations at time $s < t$ and context information at time $t$. Given history $\cH_t$, we assume $\bw_{t,a}$'s are mutually independent and satisfy
\begin{equation}
\label{eq:expectation}
\EE[\bw_{t,a} | \cH_t] = \theta_{\ast}^{\top} x_{t,a},
\end{equation}
where $\theta_{\ast}$ is an unknown $d$-dimensional vector with the assumption that $\norm{\theta_{\ast}}_2 \leq 1$ and $0 \leq \theta_{\ast}^{\top} x_{t,a} \leq 1$ for all $t, a$. 
We define random variable $\bO_t$ as the number of observed base arms in $\bA_t$, that is,
	for some $k=1,2,\ldots, \abs{\bA_t}$, 
$$
\bO_t = k, \text{ if } \bw_t(\ba_j^t)=0, \forall\, j < k \text{ and } \bw_t(\ba_k^t) = 1,
$$
or 
$$
\bO_t = \abs{\bA_t}, \text{ if }\bw_t(\ba_j^t) = 0, ~~ \forall\, j \leq \abs{\bA_t}.
$$
At the end of every time $t$, the agent observes the first $\bO_t$ items of $\bA_t$. We say that item $a$ is {\it observed} if $a = \ba_k^t$ for some $k \leq \bO_t$. 
Thus, $\cH_t$ consists of $\{x_s, \bA_s = (\ba_{1}^s,...,\ba_{\abs{\bA_s}}^s), \bO_s, \bw_s(\ba_k^s),x_t 
	\mid  k \in[\bO_s], 1 \le s<t \}$.

The agent receives some reward if the user clicks on some item. Suppose we consider the position discount: if the user clicks on the $k$-th item, then the learning agent receives reward $0 \leq \gamma_k \leq 1$. Usually the importance of the positions is decreasing: the first position is most important. So it is a reasonable assumption that
$$
1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_K \geq 0.
$$
At the end of time $t$, the learning agent observes $\bO_t, \bw_t(\ba_k^t), k \leq \bO_t$ and receives reward
$$
\br_t = \bw_t(\ba_{\bO_t}^t) \gamma_{\bO_t} = \bigvee_{k=1}^{\abs{\bA_t}} \gamma_k \bw_t(\ba_k^t).
$$
where we use the notation that $\bigvee_{k=1}^n a_k = \max_{1 \leq k \leq n} a_k$. Notice that the order of $\bA_t$ affects both the feed back and the reward.

Now let us introduce a function $f$ on $A = (a_1,...,a_{\abs{A}}) \in \cS, w = (w(1),...,w(L))$ by
\begin{align*}
&f : \cS \times [0,1]^E \to [0,1]\\
&f(A,w) = \sum_{k = 1}^{\abs{A}} \gamma_{k} \prod_{i=1}^{k-1} (1 - w(a_i)) w(a_k).
\end{align*}
Then we have $\br_t = f(\bA_t, \bw_t)$ and $\EE[\br_t] = f(\bA_t, \theta_{\ast}^{\top}x_t)$ where $x_t = (x_{t,1}, \cdots, x_{t,L}) \in \RR^{d \times L}$. Let 
\begin{align*}
A_t^{\ast} &= \argmax_{A\in \cS} f(A,\theta_{\ast}^{\top}x_t),\\
f_t^{\ast} &= f(A_t^{\ast}, \theta_{\ast}^{\top}x_t).
\end{align*}
The goal of our learning algorithm is to minimize the expected cumulative regret in $T$ steps
$$
R(T) = \EE[\sum_{t=1}^T R(t, \bA_t)].
$$
where $R(t, \bA_t) = f_t^{\ast} - f(\bA_t, \theta_{\ast}^{\top}x_t)$.

The above formulation is on the disjunctive objective, that is, at a time step the agent stops as soon as she reveals a base arm at a position $k$ in the sequence with weight $1$ and she receives reward $\gamma_k$ as the result. Similarly, we could also consider the case of conjunctive objective. We also use Bernoulli random variable $\bw_{t}(a) \in \{0,1\}$ to indicate the weight of item $a$ at time $t$ satisfying Equation \eqref{eq:expectation}). The learning agent observes all items until the first one with weight $0$. Suppose we also consider partial reward: if the $k$-th item is the first item with weight $0$, then the learning agent receives reward $\gamma_k'$; if all items have weight $1$, the learning agent receives reward $1$. The more items the learning agent reveals, the more reward the agent should receive. It is reasonable to assume that
$$
0 = \gamma'_1 \leq \gamma'_2 \leq \cdots \leq \gamma'_K \leq 1.
$$
At the end of time $t$, the learning agent observes $\bO_{\wedge, t}, \bw_t(\ba_k^t), k \leq \bO_{\wedge, t}$ and receives reward
	$\br_{\wedge, t}$:
\begin{align*}
\br_{\wedge, t} &= \begin{cases}
\gamma_{\bO_{\wedge, t}}'  &\text{if } \bw_t(\ba_{\bO_{\wedge, t}}^t) = 0,\\
1 &\text{if } \bw_t(\ba_{\bO_{\wedge, t}}^t) = 1,
\end{cases}\\
&=\begin{cases}
\gamma_{k}'  &\text{if } \bw_t(\ba_{i}^t) = 1, i < k, \bw_t(\ba_{k}^t) = 0,\\
1 &\text{if } \bw_t(\ba_{i}^t) = 1, i\leq \abs{\bA_t}.
\end{cases}
\end{align*}

If we define a function $f_{\wedge}$ on $A = (a_1, \ldots, a_{\abs{A}}) \in \cS, w = (w(1), \ldots, w(L))$ by
\begin{align*}
&f_{\wedge} : \cS \times [0,1]^E \to [0,1]\\
&f_{\wedge}(A,w) = \sum_{k = 1}^{\abs{A}} \gamma_k' (\prod_{i = 1}^{k - 1} w(a_i))(1 - w(a_k)) + \prod_{i=1}^{\abs{A}}w(a_i),
\end{align*}
then we have $\br_{\wedge, t} = f_{\wedge}(\bA_t, \bw_t)$ and $\EE[\br_{\wedge, t}] = f_{\wedge}(\bA_t, \theta_{\ast}^{\top}x_t)$ where $x_t = (x_{t,1}, \cdots, x_{t,L}) \in \RR^{d \times L}$. To simplify notations, we assume 
$$
\gamma_k' = 1 - \gamma_k.
$$
Then it holds that
\begin{equation}
\label{eq:ConDisRelation}
f_{\wedge}(A, w) = 1 - f(A, 1 - w).
\end{equation}
Let 
\begin{align*}
A_{\wedge, t}^{\ast} &= \argmax_{A\in \cS} f(A,\theta_{\ast}^{\top}x_t),\\
f_{\wedge, t}^{\ast} &= f(A_t^{\ast}, \theta_{\ast}^{\top}x_t).
\end{align*}
For the conjunctive objective, the goal of our learning algorithm is to minimize the expected cumulative regret in $T$ steps
$$
R_{\wedge}(T) = \EE[\sum_{t=1}^T R_{\wedge}(t, \bA_t)].
$$
where $R_{\wedge}(t, \bA_t) = f_{\wedge, t}^{\ast} - f_{\wedge}(\bA_t, \theta_{\ast}^{\top}x_t)$.



\section{Algorithms and Results}

\subsection{Algorithm}
	
At time $t$, we denote $\EE_t[\cdot] = \EE[\cdot | \cH_t]$, where $\cH_t$ is the history a time $t$ before the learning agent
	chooses her action. 
By Equation \eqref{eq:expectation}, we have 
$$
\EE[(\gamma_k \bw_{s,\ba_k^s}) | \cH_{s}] = \theta_*^{\top} (\gamma_k x_{s,\ba_k^s}).
$$
Using the ridge regression of data 
$$
\{(\gamma_k x_{s,\ba_k^s}, \gamma_k \bw_{s,\ba_k^s})\}_{k \in[\bO_s], s\in[t]},
$$
we get an $l^2$-regularized least-squares estimate of $\theta_*$ with regularization parameter $\lambda > 0$:
\begin{equation}
\hat{\theta}_t = (\bX_t^{\top}\bX_t + \lambda I)^{-1} \bX_t^{\top} \bY_t,
\end{equation}
where $\bX_t \in \RR^{(\sum_{s=1}^{t}\bO_s) \times d}$ is the matrix whose rows are $\gamma_k x_{s,\ba_k^s}^{\top}$ and $\bY_t$ is the column vector whose elements are $\gamma_k \bw_s(\ba_k^s)$, $k \in[\bO_s], s\in[t]$. Let
$$
\bV_t = \bX_t^{\top} \bX_t + \lambda I = \lambda I + \sum_{s=1}^{t} \sum_{k=1}^{\bO_s} \gamma_k^2 x_{s,\ba_k^s}x_{s,\ba_k^s}^{\top}.
$$
Then $\bV_t \in \RR^{d \times d}$ is a positive invertible matrix. 
In the following, we will mainly use notations for the disjunctive objective when the two cases are similar.

Next we build on a good estimate of differences between $\hat{\theta}_t$ and $\theta_*$ by Theorem 2 in \cite{abbasi2011improved}, which states the following results.
	
\begin{theorem}[\cite{abbasi2011improved}]
\label{thm:theta_estimate}
Let 
$$
\beta_{t}(\delta) = \sqrt{\log(\det(\bV_{t})) + 2 \log\left(\frac{1}{\delta}\right)} + \sqrt{\lambda}.
$$
Then for any $\delta > 0$, with probability at least $1 - \delta$, for all $t > 0$, we have
\begin{equation}
\label{eq:estimateTheta}
\norm{\hat{\theta}_t - \theta_{\ast}}_{\bV_{t}} \leq \beta_{t}(\delta).
\end{equation}
\end{theorem}

This theorem states that with high probability, the estimate $\hat{\theta}$ lies in the ellipsoid centered at $\theta_*$  with confidence radius $\beta_t(\delta)$ under $\bV_t$ norm. Building on this, we can define an upper confidence bound of the expected weight of base arm $a$ by
\begin{equation}
\label{eq:defU}
\bU_t(a) = \min\left\{\hat{\theta}_{t-1}^{\top}x_{t,a} + \beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}, 1 \right\}.
\end{equation}

The fact that $\bU_t(a)$ is an upper confidence bound of expected weight $\theta_*^{\top}x_{t,a}$ is proved in the following lemma.
\begin{lemma}
\label{lem:estimateU}
When Eq.\eqref{eq:estimateTheta} holds for time $t-1$, we have
$$
0 \leq \bU_t(a) - \theta_{\ast}^{\top}x_{t,a} \leq 2\beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}.
$$
\end{lemma}
\begin{proof}
\begin{align*}
\abs{\hat{\theta}_{t-1}^{\top}x_{t,a} - \theta_{\ast}^{\top}x_{t,a}} &\leq \norm{\hat{\theta}_{t-1} - \theta_{\ast}}_{\bV_{t-1}} \norm{x_{t,a}}_{\bV_{t-1}^{-1}} \\
&\leq \beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}.
\end{align*}
\end{proof}

Our proposed algorithm, C$^3$-UCB, is described in Algorithm \ref{alg:ConComCascade}. 
First, the learning agent 
	computes the upper confidence bounds (UCBs) $\bU_t \in [0,1]^{E}$ on the expected weights of all base arms in $E$. 
Second, she uses the computed UCBs $\bU_t$ to select an action $\bA_t$. 
Third, she plays $\bA_t$ and observes all feedback of base arms until first $0$ weight come out; the learning agent observes $\bO_t$ base arms and $\bw_t(\ba_k^t), k \in [\bO_t]$, where 
$$
\bw_t(\ba_{k}^t) = \begin{cases} 0, ~~k < \bO_t\\ 1, ~~k = \bO_t\end{cases}
$$ 
or
$$
\bw_t(\ba_k^t) = 0, k \leq \bO_t = \abs{\bA_t}.
$$
Then, the learning agent updates $\bV_t, \bX_t, \bY_t$ in order to get a newer estimate $\hat{\theta}_t$ of $\theta_*$ and new confidence radius $\beta_t(\delta)$. In this part, we use the notation $[A; B]$ to denote the matrix obtained by stacking A and B vertically like
 $\begin{pmatrix} A\\ B\end{pmatrix}$.

\begin{algorithm}
\caption{C$^3$-UCB}
\label{alg:ConComCascade}
\begin{algorithmic}[1]
\STATE {//Initialization}
\STATE {Parameters: $1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_K \geq 0$}
\STATE {Parameters: $\delta > 0, \lambda \geq C_\gamma = \sum_{k=1}^{K} \gamma_k^2$}
\STATE {$\hat{\theta}_0 = 0, \beta_0(\delta) = 1, \bV_0 = \lambda I, \bX_0=\emptyset, \bY_0=\emptyset$}
%\STATE{}

\FORALL {$t=1,2,\ldots$}
\STATE {Obtain context $x_{t,a}$ for all $a\in E$}
%\STATE{}

\STATE {//Compute UCBs}
\STATE {$\forall a\in E: $}
\STATE {$\bU_t(a) \leftarrow \min\{\hat{\theta}_{t-1}^{\top}x_{t,a} + \beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}, 1\}$}
%\STATE {}
		
\STATE {//Choose action $\bA_t$ using UCBs $\bU_t$}
\STATE {$\bA_t=(\ba_1^t,...,\ba_{\abs{\bA_t}}^t) \leftarrow \argmax_{A \in \cS} f(A, \bU_t)$}
\STATE {//In the conjunctive case, use $f_{\wedge}$ in stead of $f$}
\STATE {Play $\bA_t$ and observe $\bO_t, \bw_t(\ba_k^t), k\in[\bO_t]$}
%\STATE {}

\STATE {//Update statistics}
\STATE {$\bV_{t} \leftarrow \bV_{t-1} + \sum_{k=1}^{\bO_t} \gamma_k^2 x_{t, \ba_k^t}x_{t, \ba_k^t}^{\top}$}
\STATE {$\bX_t \leftarrow [\bX_{t-1}; ~\gamma_1 x_{t, \ba_1^t}^{\top};  ~\ldots; ~\gamma_{\bO_t} x_{t, \ba_{\bO_t}^t}^{\top}]$}
\STATE {$\bY_t \leftarrow [\bY_{t-1}; ~\gamma_1 \bw(\ba_1^t); ~\ldots; ~\gamma_{\bO_t} \bw_t(\ba_{\bO_t}^t)]$}
\STATE {$\hat{\theta}_t \leftarrow (\bX_t^\top \bX_t + \lambda I)^{-1} \bX_{t}^\top \bY_t$}
\STATE {$\beta_{t}(\delta) \leftarrow \sqrt{\log(\det(\bV_{t})) + 2 \log(\frac{1}{\delta})} + \sqrt{\lambda}$}
			
\ENDFOR {~~$t$}
\end{algorithmic}
\end{algorithm}
	

\subsection{Results}

To state our main theorem, we need some definitions. Let
$$
f_{\wedge}^* = \max_{1 \leq t \leq T} f_{\wedge, t}^{\ast}
$$
denote the best expected reward of all time under the conjunctive objective.
Let $p_{t, A}$ (resp., $p_{\wedge, t, A}$) be the probability that all base arms of $A = (a_1, \ldots, a_{\abs{A}})$ have been observed under the disjunctive objective (resp., conjunctive objective) when under the context of time $t$, that is 
\begin{align*}
&p_{t, A} = \prod_{k=1}^{\abs{A}-1} (1 - \theta_{\ast}^{\top} x_{t, a_k^t}),\\
&p_{\wedge, t, A} = \prod_{k=1}^{\abs{A}-1} (\theta_{\ast}^{\top} x_{t, a_k^t}).
\end{align*}
We also define $p^* = \min_{1 \leq t \leq T} \min_{A \in \cS} ~ p_{t, A}$ to be the minimal probability over all time 
	that an action could have all base arms observed under the 
disjunctive objective. The following is the main theorem on the regret achieved by
	our C$^3$-UCB algorithm, for both the disjunctive and conjunctive objectives.
\begin{theorem}
\label{thm:main}
Suppose $1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_{K} \geq 0$ and 
$\gamma_k' = 1 - \gamma_k, \gamma'_K \leq \frac{1}{4}f_{\wedge}^{\ast}$.
Let regularization parameter $\lambda$ satisfy $\lambda \geq C_\gamma$,
	where $C_\gamma = \sum_{k=1}^{K} \gamma_k^2$. 
Then for any $\delta > 0$, with probability at least $1 - \delta$, the cumulative expected regrets of our algorithm, C$^3$-UCB for both the conjunctive and disjunctive objectives, satisfy
\begin{align}
R(T) &\le \frac{\sqrt{2}}{p^*} \sqrt{TKd\log(\lambda + C_\gamma T/d)}  \nonumber \\
&\qquad \cdot \left(\sqrt{d\log(\lambda + C_\gamma T/d) + 2\log\left(\frac{1}{\delta}\right)} + \sqrt{\lambda}\right) \nonumber \\
&=O\left(\frac{d}{p^*} \sqrt{TK} \log T\right),
\end{align}
\begin{align}
R_{\wedge}(T) &\le \frac{\sqrt{128}}{f_{\wedge}^{\ast}} \sqrt{TKd\log(\lambda + C_\gamma T/d)} \nonumber \\
&\qquad \cdot \left(\sqrt{d\log(\lambda + C_\gamma T/d) + 2\log\left(\frac{1}{\delta}\right)} + \sqrt{\lambda}\right) \nonumber \\
&=O\left(\frac{d}{f_{\wedge}^{\ast}}\sqrt{TK} \log T\right).
\end{align}
%where
%\begin{align*}
%C_\gamma = \sum_{k=1}^{K} \gamma_k^2 = \sum_{k=1}^{K} (1 - \gamma_k')^2 \in [1, K].
%\end{align*}
\end{theorem}

\wei{Is the $\log$ in the above theorem with base $2$? 
	Other UCB regret bounds usually has $\ln$. Please double check if here
	indeed we use $\log$ of base 2. }
To prove Theorem \ref{thm:main}, we could start from estimating $\EE[\Delta_{t, \bA_t}|\cH_t]$ for every time $t$. In fact, we can derive the following lemma.
\wei{I did not see $\Delta_{t,\bA_t}$ defined before.}
\begin{lemma}
\label{lem:DeltaEstimate}
For any time $t$ and $A = (a_1, \ldots, a_{\abs{A}})$, if $f(A_t^*, \bU_t) \leq f(A, \bU_t)$, then we have
$$
\Delta_{t,A} \leq \sum_{k=1}^{\abs{A}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t,a_k}}_{\bV_{t-1}^{-1}};
$$
if $f_{\wedge}(A_t^*, \bU_t) \leq f_{\wedge}(A, \bU_t)$, then we have
$$
\Delta_{\wedge, t,A} \leq \sum_{k=1}^{\abs{A}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t,a_k}}_{\bV_{t-1}^{-1}}.
$$
\end{lemma}

So if the chosen action $\bA_t$ is not suboptimal, which means
$$
f(A_t^*, \bU_t) \leq f(\bA_t, \bU_t), ~~ f_{\wedge}(A_t^*, \bU_t) \leq f_{\wedge}(\bA_t, \bU_t).
$$
Then we can derive an estimate of $\Delta_{t, \bA_t}$ and $\Delta_{\wedge, t, \bA_t}$, however, in terms of all base arms in $\bA_t$. Only when the learning agent has observed all base arms of $\bA_t$, we can use such an estimate, which means we are able to estimate
\begin{align*}
&~~\EE_t[\Delta_{t, \bA_t}] \\
&= \EE_t[\Delta_{t, \bA_t}(1/p_{t, \bA_t}) \bOne\{\Delta_{t, \bA_t} > 0, \bO_t \geq \abs{\bA_t}\}],\\
&~~\EE_t[\Delta_{\wedge, t, \bA_t}] \\
&= \EE_t[\Delta_{\wedge, t, \bA_t}(1/p_{\wedge, t, \bA_t}) \bOne\{\Delta_{\wedge, t, \bA_t} > 0, \bO_{\wedge, t} \geq \abs{\bA_t}\}].
\end{align*}

In real problems involved with recommendations, the click rate is a very small number. Thus for the problem with disjunctive objective, $p^*$ is relatively high and can be regarded as a constant. So we have $p_{t, \bA_t} \geq p^*$. For the problem with conjunctive objective, $p_{\wedge, t, \bA_t}$ might be small. Thus we need the following lemma to find a good substitution of $\bA_t$.

\begin{lemma}
\label{lem:prefixExist}
Let $B_k = (\ba_1^t, \ldots, \ba_k^t), k \leq \abs{A}$ be a prefix of $\bA_t$. Then there exists a prefix $\bB_t$ of $\bA_t$ such that
$$
p_{\wedge, t, \bB_t} \geq \frac{1}{2} f_{\wedge, t}^* - \gamma_{\abs{\bB_t}}, \qquad \Delta_{\wedge, t, \bB_t} \geq \frac{1}{2} \Delta_{\wedge, t, \bA_t}.
$$
\end{lemma}
From this lemma, we can use the prefix $\bB_t$ to substitue $\bA_t$ because $p_{\wedge, t, \bB_t}$ is large enough and $\Delta_{\wedge, t, \bB_t}$ will not be too small. Then follow the above deductions, we can get a good estimate for $\EE[\Delta_{\wedge, t, \bA_t}|\cH_{t-1}]$ in terms of $\norm{x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}$. Once we have estimated $\norm{x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}$ and $\beta_{t-1}(\delta)$, we can get an estimate for $\EE[\Delta_{\wedge, t, \bA_t}|\cH_{t-1}]$, thus for $R_{\wedge}(T)$. We leave all details in the next subsection.


\subsection{Proofs}

\begin{lemma}
\label{lem:increasing} 
Suppose $1 \geq \gamma_1 \geq \cdots \geq \gamma_K \geq 0$ and $0 \leq \gamma_1' \leq \cdots \leq \gamma_K' \leq 1$. Then $f(A, w)$ and $f_{\wedge}(A, w)$ are increasing with respect to $w$; equivalently, if $0 \leq w \leq w' \leq 1$ as column vectors in $\RR^L$, then for any $A \in \prod^{\leq K}(E)$, it holds that
$$
f(A, w) \leq f(A, w'), ~~ f_{\wedge}(A, w) \leq f_{\wedge}(A, w').
$$
\end{lemma}
\begin{proof}
It suffices to prove when $A = (1, \ldots, m)$, where $1 \leq m \leq K$. Because for $1 \leq k \leq m$,
\begin{align*}
&\gamma_{k} - \sum_{i=k+1}^m \gamma_i \prod_{j = k + 1}^{i - 1} (1 - w_j') w_i'\\
\geq &~\gamma_k [1 - \sum_{i=k+1}^m \prod_{j=k+1}^{i-1}(1 - w_j') w_i']\\
\geq &~\gamma_{k} \cdot 0 = 0,
\end{align*}
then
\begin{align*}
&\gamma_k w_k + (1 - w_k)\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i'\\
\leq &\gamma_k w_k' + (1 - w_k')\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i'\\
\end{align*}
Therefore, 
\begin{align*}
& f(A; w_1, \dots, w_k, w_{k+1}', \dots, w_m')\\
&=\sum_{i=1}^{k-1} \gamma_i \prod_{j=1}^{i-1}(1 - w_j) w_i + \prod_{j=1}^{k-1}(1 - w_j) \\
&\qquad \cdot [\gamma_k w_k + (1 - w_k)\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i']\\
&\leq \sum_{i=1}^{k-1} \gamma_i \prod_{j=1}^{i-1}(1 - w_j) w_i + \prod_{j=1}^{k-1}(1 - w_j) \\
&\qquad \cdot [\gamma_k w_k' + (1 - w_k')\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i']\\
&=f(A; w_1, \ldots, w_{k-1}, w_{k}', \ldots, w_m').
\end{align*}
By letting $\gamma_k = 1 - \gamma_k'$ and the relation (\ref{eq:ConDisRelation}), the increasing property of $f_{\wedge}$ is obtained directly.
\end{proof}

\begin{lemma}
\label{lem:estimateTech}
Suppose $0 \leq \gamma_k, \gamma_k' \leq 1, k \in [K]$. Let $0 \leq p_1, \ldots, p_m \leq 1$, $u_1, \ldots, u_m \geq 0$ and $w_i = \min\{p_i + u_i, 1\}, 1 \leq i \leq m$. Then
\begin{align*}
&f(A^{(m)}, w) \leq f(A^{(m)}, p) + \sum_{k=1}^{m} \gamma_k u_k,\\
&f_{\wedge}(A^{(m)}, w) \leq f_{\wedge}(A^{(m)}, p) + \sum_{k=1}^{m} (1 - \gamma_k') u_k,
\end{align*}
where $A^{(m)} = (1, 2, \ldots, m), w = (w_1, \ldots, w_m)$ and $p = (p_1, \ldots, p_m)$.
\end{lemma}
\begin{proof}
We prove this by induction. It holds obviously for $m = 1$.
\begin{align*}
&~~f(A^{(m+1)}, w)\\
&= f(A^{(m)}, w) + \gamma_{m+1}\prod_{k=1}^m(1 - w_k) w_{m+1}\\
&\leq f(A^{(m)}, w) +  \gamma_{m+1} \prod_{k=1}^m(1 - p_k) (p_{m+1} + u_{m+1})\\
&\leq f(A^{(m)}, p) + \sum_{k=1}^m \gamma_k u_k \\
&\qquad + \gamma_{m+1} \prod_{k=1}^m(1 - p_k) p_{m+1} + \gamma_{m+1} u_{m+1}\\
&= f(A^{(m+1)}, p) + \sum_{k=1}^{m+1} \gamma_k u_k
\end{align*}
\begin{align*}
&~~f_{\wedge}(A^{(m+1)}, w) \\
&= f_{\wedge}(A^{(m)}, w) -\prod_{k=1}^{m} w_k \\
&\qquad+ \gamma_{m+1}' (\prod_{k=1}^{K} w_k) (1 - w_{K+1})+ \prod_{k=1}^{K+1} w_k\\
&= f_{\wedge}(A^{(m)}, w) - (1 - \gamma_{m+1}') (\prod_{k=1}^{m} w_k) (1 - w_{m+1})\\
&\leq f_{\wedge}(A^{(m)}, w) - (1 - \gamma_{m+1}') (\prod_{k=1}^{m} p_k) (1 - w_{m+1})\\
&\leq f_{\wedge}(A^{(m)}, w) -(1 - \gamma_{m+1}') (\prod_{k=1}^{m} p_k)  (1 - p_{m+1} - u_{m+1})\\
&\leq f_{\wedge}(A^{(m)}, p) +  \sum_{k=1}^{m} (1 - \gamma_k') u_k \\
&\qquad - (1 - \gamma_{m+1}') (1 - p_{m+1}) \prod_{k=1}^{m} p_k + (1 - \gamma_{m+1}') u_{m+1}\\
&\leq f_{\wedge}(A^{(m+1)}, p) + \sum_{k=1}^{m+1} (1 - \gamma_k') u_k
\end{align*}
\end{proof}

\begin{proof}[ of Lemma \ref{lem:DeltaEstimate}]
By Lemma \ref{lem:estimateU}, $\theta_{\ast}^{\top}x_t \leq \bU_t$. Then by Lemma \ref{lem:increasing},
$$
f_t^{\ast} = f(A_t^{\ast}, \theta_{\ast}^{\top}x_t) \leq f(A_t^{\ast}, \bU_t).
$$
By Lemma \ref{lem:estimateTech} and the definition of $\bU_t$ \eqref{eq:defU},
$$
f(A, \bU_t) \leq f(A, \theta_{\ast}^{\top}x_t) + \sum_{k=1}^{\abs{A}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t, a_k^t}}_{\bV_{t-1}^{-1}}.
$$
Because $f(A_t^{\ast}, \bU_t) \leq f(A, \bU_t)$, then 
$$
f_t^{\ast} \leq f(A, \theta_{\ast}^{\top}x_t) + \sum_{k=1}^{\abs{A}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t, a_k^t}}_{\bV_{t-1}^{-1}}.
$$
Therefore,
$$
\Delta_{t, A} = f_t^{\ast} - f(A, \theta_{\ast}^{\top}x_t) \leq \sum_{k=1}^{\abs{A}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t, a_k^t}}_{\bV_{t-1}^{-1}}.
$$
The result for the disjunctive case can be deduced similarly.
\end{proof}

\begin{lemma}
\label{lem:prefixRelation}
Suppose 
% $1 \geq \gamma_1 \geq \cdots \geq \gamma_K \geq 0$ and 
$0 \leq \gamma_1' \leq \cdots \leq \gamma_K' \leq 1$. Let $A = (a_1, ..., a_{\abs{A}})$. 
% For the conjunctive case, it holds that
% $$
% p_A \geq 1 - \frac{f(A, w)}{\gamma_{\abs{A}}}.
% $$
Let $B_k = (a_1, ..., a_k), k \leq \abs{A}$ be a prefix of $A$. Then for the problem with conjunctive objective and $k < \abs{A}$, we have the following properties:
\begin{itemize}
\item[(1)] $f_{\wedge}(A, w) \leq \gamma_{\abs{A}}' + (1 - \gamma_{\abs{A}}') p_{\wedge, A}$;
\item[(2)] $f_{\wedge}(B_{k+1}, w) \leq f_{\wedge}(B_k, w)$;
\item[(3)] $p_{\wedge, B_{k+1}} \leq p_{\wedge, B_k}$;
\item[(4)] $f_{\wedge}(B_k, w) \leq \gamma_{k+1}' + (1 - \gamma_{k+1}') p_{B_{k+1}}$.
\end{itemize}
\end{lemma}

\begin{proof}
% It suffices to prove these properties when $A = A^{(m)}$ and $w = (w_1, \ldots, w_m)$. First, for the conjunctive objective,
% $$
% f(A, w) \geq \gamma_{m} (1 - p_{A} + p_{A} w_{m}) \geq \gamma_{m} (1 - p_{A}).
% $$
% Then for the disjunctive objective,
\begin{itemize}
\item[(1)]
\begin{align*}
f_{\wedge}(A, w) &\leq \gamma_m'(1 - p_{\wedge, A} w_m) + p_{\wedge, A} w_m \\
&\leq \gamma_m' + (1 - \gamma_m') p_{\wedge, A};
\end{align*}

\item[(2)]
\begin{align*}
&f_{\wedge}(B_k, w) - f_{\wedge}(B_{k+1}, w)\\
&=\prod_{i=1}^{k}w_i - \gamma_{k+1}' (\prod_{i=1}^{k}w_i) (1 - w_{k+1}) - (\prod_{i=1}^{k}w_i) w_{k+1}\\
&=(1 - \gamma_{k+1}') (\prod_{i=1}^{k}w_i) (1 - w_{k+1}) \geq 0;
\end{align*}

\item[(3)]
$p_{\wedge, B_{k+1}} = \prod_{i=1}^{k} w_i \leq \prod_{i=1}^{k-1} w_i = p_{\wedge, B_k}$;

\item[(4)]
\begin{align*}
f_{\wedge}(B_k, w) &\leq \gamma_{k}' (1 - p_{B_{k+1}}) + p_{B_{k+1}}\\
&\leq \gamma_{k+1}' (1 - p_{B_{k+1}}) + p_{B_{k+1}} \\
&= \gamma_{k+1}' + (1 - \gamma_{k+1}') p_{B_{k+1}}
\end{align*}
\end{itemize}
\end{proof}

\begin{lemma}
Suppose $0 \leq \gamma_1' \leq \cdots \leq \gamma_K' \leq 1$. Let $A = (a_1, ..., a_{\abs{A}})$. For the time $t$ and the conjunctive objective, there exists a prefix $B$ of $A$ such that 
$$
p_{\wedge, B} \geq \frac{1}{2}f_{\wedge, t}^{\ast} - \gamma_{\abs{B}}', \qquad \Delta_{\wedge, t, B} \geq \frac{1}{2}\Delta_{\wedge, t, A}.
$$ 
\end{lemma}
\begin{proof}
If $f_{\wedge}(A, w) \geq \frac{1}{2} f_{\wedge, t}^{\ast}$, then by Lemma \ref{lem:prefixRelation},
$$
p_{\wedge, A} \geq \frac{1}{2}f_{\wedge, t}^{\ast} - \gamma_{\abs{A}}'.
$$

Suppose $f_{\wedge}(A, w) \leq \frac{1}{2} f_{\wedge, t}^{\ast}$. Let
$$
x_k = f_{\wedge}(B_k,w), ~~ y_k = \gamma_k' + (1 - \gamma_k')p_{\wedge, B_k}, ~~I_k = [x_k, y_k]
$$
Then by Lemma \ref{lem:prefixRelation}, we have $x_k \leq y_k$, $x_{k+1} \leq x_k \leq y_{k+1}$. Therefore, $I_k$ is indeed an interval and $I_k \cap I_{k+1} \neq \emptyset$. Also, $x_{\abs{A}} = f_{\wedge}(A, w)$ and $y_1 = 1$. Thus
$$
[f_{\wedge}(A,w), 1] = \bigcup_{k=1}^{\abs{A}} I_k.
$$
Then there exists a $k$ such that $\frac{1}{2}f_{\wedge, t}^{\ast} \in I_k$:
$$
f_{\wedge}(B_k,w) \leq \frac{1}{2}f_{\wedge, t}^{\ast} \leq \gamma_k' + (1 - \gamma_k')p_{\wedge, B_k}
$$
Therefore, the results are derived.
\end{proof}


\begin{proposition}
Let $1 \geq \gamma_1 \geq \cdots \geq \gamma_K \geq 0$ and $\gamma_k' = 1 - \gamma_k$. Suppose the equation (\ref{eq:estimateTheta}) holds for all time $t$. Then
$$
E[\Delta_{t, \bA_t}|\cH_t] \leq \frac{1}{p^*} \beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}
$$
If we have
% $$
% \gamma_K \geq (1+\frac{1}{n_0})f^{\ast}, \qquad 
% $$
$$
\gamma_K' \le \frac{1}{4} f_{\wedge}^{\ast},
$$
then
\begin{align*}
% &E[\Delta_{t, \bA_t}|\cH_t] \leq (n_0+1) \beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}},\\
&E[\Delta_{\wedge, t, \bA_t}|\cH_t] \leq \frac{8}{f_{\wedge}^{\ast}} \beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}},
\end{align*}
where $f^{\ast} = \max_{t} f_t^{\ast}, f_{\wedge}^{\ast} = \max_{t} f_{\wedge, t}^{\ast}$.
\end{proposition}
\begin{proof}
For the disjunctive case,  $p_{\bA_t} \geq p^*$ by definition and the result can be derived directly.
% $p_{\bA_t} \geq \frac{1}{n_0 + 1}$ by lemma \ref{lem:prefixRelation} and the result can be derived directly. 

For the conjunctive case, there exists a prefix $\bB_t$ of $\bA_t$ such that $p_{\wedge, \bB_t} \geq \frac{1}{4}f_{\wedge, t}^*$ and $\Delta_{\wedge, \bB_t} \geq \frac{1}{2}\Delta_{\wedge, A}$. Then we have
$$
\EE_t[\Delta_{\wedge, t, \bA_t}] \leq \frac{8}{f_{\wedge, t}^*} \EE_t[\Delta_{\wedge, t, \bB_t} \bOne\{\Delta_{\wedge, t, \bB_t} > 0, \bO_t \geq \abs{\bB_t}\}
$$
Because 
$$
f_{\wedge}(A_t^*, \theta_*^{\top}X_t) \leq f_{\wedge}(A_t^*,\bU_t) \leq f_{\wedge}(\bA_t,\bU_t) \leq f_{\wedge}(\bB_t,\bU_t)
$$
by Lemma \ref{lem:increasing} and
$$
f_{\wedge}(\bB_t,\bU_t) \leq f_{\wedge}(\bB_t, \theta_*^{\top}x_t) + \sum_{k=1}^{\abs{\bB_t}}\gamma_k\beta_{t-1}(\delta)\norm{x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
by Lemma \ref{lem:estimateTech}, we have
$$
\Delta_{\wedge, t, \bB_t} \leq \sum_{k=1}^{\abs{\bB_t}}\beta_{t-1}(\delta)\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
\end{proof}
	
\begin{lemma} % det(V_t)
$\det(\bV_t) \leq (\lambda + C_\gamma t/d)^d.$
\end{lemma}
\begin{proof}
We have $\det(\bV_t) \leq (\trace(\bV_t)/d)^d$ and
\begin{align*}
&\trace(\bV_t)\\
& = \trace(\lambda I) + \sum_{s=1}^t \sum_{k=1}^{\bO_s} \gamma_k^2 \trace(x_{s,\ba_k^s} x_{s,\ba_k^s}^{\top})\\	
& = d \lambda + \sum_{s=1}^t \sum_{k=1}^{\bO_s} \gamma_k^2 \norm{x_{s,\ba_k^s}}_2^2\\
& \leq d \lambda + \sum_{s=1}^t\sum_{k=1}^{K}\gamma_k^2 = d \lambda + t C_\gamma.
\end{align*}
\end{proof}

\begin{lemma} % technical lemma for det
Let $x_i \in \RR^{d \times 1}, 1 \leq i \leq n$. Then we have
$$
\det(I + \sum_{i=1}^n x_i x_i^{\top}) \geq 1 + \sum_{i=1}^n \norm{x_i}_2^2.
$$
\end{lemma}
\begin{proof}
Denote the eigenvalues of $I + \sum_{i=1}^n x_i x_i^{\top}$ by $1+\alpha_1,...,1+\alpha_d$ with $\alpha_j \geq 0$, $1\leq j\leq d$. Then
\begin{align*}
&\det(I + \sum_{i=1}^n x_i x_i^{\top})= \prod_{j=1}^d (1 + \alpha_j)\\
&\geq 1 +\sum_{j=1}^d \alpha_j =1-d + \sum_{i=1}^d (1+\alpha_i) \\
&=1-d + \trace(I + \sum_{i=1}^n x_i x_i^{\top})= 1-d + d + \sum_{i=1}^n \norm{x_i}_2^2\\
&=1 + \sum_{i=1}^n \norm{x_i}_2^2.
\end{align*}
\end{proof}

\begin{lemma}
If $\lambda \geq C_\gamma$, then
$$
\sum_{s=1}^t \sum_{k=1}^{\bO_s} \norm{\gamma_k x_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \leq 2\log(\det(\bV_t)).
$$
\end{lemma}
\begin{proof}
\begin{align*}
&\det(\bV_t) = \det(\bV_{t-1} + \sum_{k=1}^{\bO_t} (\gamma_k X_{t,\ba_k^{t}})(\gamma_k X_{t, \ba_k^{t}}^{\top}))\\
&=\det(\bV_{t-1})\\
&~~~\cdot \det(I + \sum_{k=1}^{\bO_t} \gamma_k \bV_{t-1}^{-1/2}x_{t,\ba_{k}^{t}} (\gamma_k \bV_{t-1}^{-1/2}x_{t,\ba_{k}^{t}})^{\top})\\
&\geq \det(\bV_{t-1}) (1 + \sum_{k=1}^{\bO_t} \norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2)\\
&\geq \det(\lambda I)\prod_{s=1}^{t}(1 + \sum_{k=1}^{\bO_s} \norm{\gamma_k x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)
\end{align*}
Because
$$
\norm{\gamma_k x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2 \leq \norm{\gamma_k x_{s,\ba_k^s}}_2^2/\lambda_{\min}(\bV_{s-1}) \leq \gamma_k^2 /\lambda,
$$
we have 
$$
\sum_{k=1}^{\bO_s} \norm{\gamma_k x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2 \leq \frac{1}{\lambda} \sum_{k=1}^{K} \gamma_k^2 = C_\gamma /\lambda \leq 1
$$
Using the fact that $ 2\log(1+u) \geq u$ for any $u \in [0,1]$, we get
\begin{align*}
&\sum_{s=1}^t \sum_{k=1}^{\bO_s}\norm{\gamma_k x_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \\
&\leq 2\sum_{s=1}^t\log(1 + \sum_{k=1}^{\bO_s} \norm{\gamma_k x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)\\
&\leq 2(\log(\det(\bV_t)) - \log(\det(\lambda I))) < 2\log(\det(\bV_t))
\end{align*}
\end{proof}

\begin{proof}[ of Theorem \ref{thm:main}]
Suppose inequality (\ref{eq:estimateTheta}) holds for all time $t$. 

For the conjunctive case,
\begin{equation}
\begin{split}
&R(T) =\sum_{t=1}^{T} \EE_{t}[\Delta_{t, \bA_t}] \\
&\leq \EE[\sum_{t=1}^{T} \frac{1}{p^*} \EE_t[\beta_{t-1}(\delta) \sum_{k=1}^{\bO_t}\norm{\gamma_k X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}]]\\
&\leq \EE[\frac{1}{p^*} \beta_T(\delta) \sqrt{(\sum_{t=1}^{T} \bO_t) \EE[\sum_{t=1}^{T} \sum_{k=1}^{\bO_t}\norm{\gamma_k X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]} ]  \\
&\leq \EE[\frac{1}{p^*} (\sqrt{\log(\det(\bV_t)) + 2\log(\frac{1}{\delta})} + \sqrt{\lambda}) \\
&\qquad \qquad \cdot \sqrt{TK \cdot 2\log(\det(\bV_t))} ]\\
&\leq \frac{\sqrt{2}}{p^*} (\sqrt{d\log(\lambda + C_\gamma T/d) + 2\log(\frac{1}{\delta})} + \sqrt{\lambda})\\
&\qquad \qquad \cdot \sqrt{TKd\log(\lambda + C_\gamma T/d)} ]
\end{split}
\end{equation}

For the disjunctive case, 
\begin{equation}
\begin{split}
&R(T) =\sum_{t=1}^{T} \EE_{t}[\Delta_{\wedge, t, \bA_t}] \\
&\leq \EE[\sum_{t=1}^{T} \frac{8}{f_{\wedge}^{\ast}} \EE_t[\beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}] \\
&\leq \EE[\frac{8}{f_{\wedge}^{\ast}} \beta_{T}(\delta) \sum_{t=1}^{T} \sum_{k=1}^{\bO_t}\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}]\\
&\leq \EE[\frac{8}{f_{\wedge}^{\ast}} \beta_{T}(\delta) \sqrt{(\sum_{t=1}^{T} \bO_t) \EE_t[\sum_{t=1}^{T} \sum_{k=1}^{\bO_t}\norm{\gamma_k X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]}]\\
&\leq \EE[\frac{\sqrt{128}}{f_{\wedge}^{\ast}} (\sqrt{\log(\det(\bV_t)) + 2\log(\frac{1}{\delta})} + \sqrt{\lambda}) \\
&\qquad \qquad \cdot \sqrt{TK \cdot 2\log(\det(\bV_t))} ]\\
&\leq \frac{\sqrt{128}}{f_{\wedge}^{\ast}} (\sqrt{d\log(\lambda + C_\gamma T/d) + 2\log(\frac{1}{\delta})} + \sqrt{\lambda})\\
&\qquad \qquad \cdot \sqrt{TKd\log(\lambda + C_\gamma T/d)} ]
\end{split}
\end{equation}
\end{proof}
	
	
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}
	
\bibliography{cascade_reference}
\bibliographystyle{icml2015}
	
\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
