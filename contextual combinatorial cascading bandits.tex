%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% my settings
\usepackage{amssymb,amsfonts,amsmath,bbm}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage[all]{xy}
\usepackage{empheq}
\usepackage{bm}
\usepackage{algorithm,algorithmic}
\usepackage{multirow}
\usepackage{ifthen}
\usepackage[usenames]{color}
\usepackage[usenames,dvipsnames]{xcolor}


\newcommand{\EE}{\mathbb{E}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bOne}{\mathbbm{1}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\trace}{\mathrm{trace}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\| #1 \|}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}%[chapter]
\newtheorem{definition}[theorem]{Definition}%[chapter]
\newtheorem{corollary}[theorem]{Corollary}%[chapter]
\newtheorem{lemma}[theorem]{Lemma}%[chapter]
\newtheorem{example}[theorem]{Example}
\newenvironment{proof}{\noindent {\textbf{Proof. }}}{$\Box$ \medskip}

\newcommand{\compilehidecomments}{false}

% Macro for comments:
\ifthenelse{ \equal{\compilehidecomments}{true} }{%
	\newcommand{\wei}[1]{}
	\newcommand{\yang}[1]{}
	\newcommand{\yajun}[1]{}
}{
\newcommand{\wei}[1]{{\color{blue!50!black}  [\text{Wei:} #1]}}
\newcommand{\shuai}[1]{{\color{brown!60!black} [\text{Shuai:} #1]}}
\newcommand{\shengyu}[1]{{\color{green!50!black} [\text{Shengyu:} #1]}}
}


% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Contextual Combinatorial Cascading Bandits}

\begin{document} 
	
\twocolumn[
\icmltitle{Contextual Combinatorial Cascading Bandits}
	
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute, 314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute, 27182 Exp St., Toronto, ON M6H 2T1 CANADA}
	
% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}
	
\vskip 0.3in
]
	
\begin{abstract} 
The purpose of this document is to provide both the basic paper template and submission guidelines.
\end{abstract} 
	
\section{Introduction}
	
\section{Related Works}
	
\section{Problem Formulation}

We formulate the problem of {\em contextual combinatorial cascading bandit} as follows. Suppose we have a finite set of $L$ ground items,  $E=\{1,...,L\}$, also referred to as {\em base arms}. 
Let $\prod^k=\{(a_1,...,a_k): a_1,...,a_k \in E, a_i \neq a_j \text{ for any } i \neq j\}$ be the set of all $k$-tuples of distinct items from $E$. We also refer to each of such tuples as an {\em action}.
Let $\cS \subseteq \cup_{k=1}^K \prod^{k}$ be the set of feasible actions with length no more than $K$.

\wei{We should unify terms, such as agent, user, player, etc. Also I suggest to refer to the agent as she, her.
Also for base arms, should we call them base arms, or items? their outcome as weights, or reward? It is fine to use up to two
terms interchangibly, but if so, we need to say it clearly at the first time the term is referred to.
Otherwise, we stick to one term.}
At time $t$, feature vectors $x_{t,a} \in \RR^{d \times 1}$ with $\norm{x_{t,a}}_2 \leq 1$ for every basic arm $a \in E$ are revealed to the learning agent; the feature vectors combine both the information of the user and the corresponding basic arm. Then the learning agent recommends a feasible solution $\bA_t=(\ba_{1}^t,...,\ba_{\abs{\bA_t}}^t) \in \cS$ to the user. 
\wei{Why using $\ba_{1}^t$, not $a_1^t$?}
Starting from the first item, the user checks the items one by one. The checking process stops if the user clicks on one item or has checked all items without clicking anyone. We use Bernoulli random variable $\bw_{t}(a) \in \{0,1\}$ to indicate whether the user has clicked on item $a$ or not at time $t$. 

Assume $\bw_{t,a}$'s are mutually independent and satisfy
\begin{equation}
\label{eq:expectation}
\EE[\bw_{t,a} | x_{t,a}] = \theta_{\ast}^{\top} x_{t,a},
\end{equation}
where $\theta_{\ast}$ is an unknown $d$-dimensional vector with the assumption that $\norm{\theta_{\ast}}_2 \leq 1$ and $0 \leq \theta_{\ast}^{\top} x_{t,a} \leq 1$ for all $t, a$. At the end of every time $t$, the agent observes the first $\bO_t$ items of $\bA_t$ where 
$$
\bO_t \leq \abs{\bA_t}, \bw_t(\ba_k^t) = \begin{cases} 0, ~~k < \bO_t \\ 1, ~~k = \bO_t \end{cases}
$$ 
\wei{The above definition of $\bO_t$ looks indirect to me. What about "For some $k=1,2,\ldots, |\bA_t|$, $\bO_t = k$
	if for all $j< k$, $\bw_t(\ba_j^t)=0$ and $\bw_t(\ba_k^t)=1$.}
or 
$$
\bO_t = \abs{\bA_t}, \bw_t(\ba_k^t) = 0, ~~ \forall\, k \leq \abs{\bA_t}.
$$
We say that item $a$ is {\it observed} if $a = \ba_k^t$ for some $k \leq \bO_t$. 

The agent receives some reward if the user clicks on some item. Suppose we consider the position discount: if the user clicks on the $k$-th item, then the learning agent receives reward $0 \leq \gamma_k \leq 1$. Usually the importance of the positions is decreasing: the first position is most important. So it is a reasonable assumption that
$$
1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_K \geq 0.
$$
At the end of time $t$, the learning agent observes $\bO_t, \bw_t(\ba_k^t), k \leq \bO_t$ and receives reward
$$
\br_t = \bw_t(\ba_{\bO_t}^t) \gamma_{\bO_t} = \bigvee_{k=1}^{\abs{\bA_t}} \gamma_k \bw_t(\ba_k^t).
$$
where we use the notation that $\bigvee_{k=1}^n a_k = \max_{1 \leq k \leq n} a_k$. Notice that the order of $\bA_t$ affects both the feed back and the reward.

Now let us introduce a function $f$ on $A = (a_1,...,a_{\abs{A}}) \in \cS, w = (w(1),...,w(L))$ by
\begin{align*}
&f : \cS \times [0,1]^E \to [0,1]\\
&f(A,w) = \sum_{k = 1}^{\abs{A}} \gamma_{k} \prod_{i=1}^{k-1} (1 - w(a_i)) w(a_k).
\end{align*}
Then we have $\br_t = f(\bA_t, \bw_t)$ and $\EE[\br_t] = f(\bA_t, \theta_{\ast}^{\top}x_t)$ where $x_t = (x_{t,1}, \cdots, x_{t,L}) \in \RR^{d \times L}$. Let 
\begin{align*}
A_t^{\ast} &= \argmax_{A\in \cS} f(A,\theta_{\ast}^{\top}x_t),\\
f_t^{\ast} &= f(A_t^{\ast}, \theta_{\ast}^{\top}x_t).
\end{align*}
The goal of our learning algorithm is to minimize the expected cumulative regret in $T$ steps
$$
R(T) = \EE[\sum_{t=1}^T R(t, \bA_t)].
$$
where $R(t, \bA_t) = f_t^{\ast} - f(\bA_t, \theta_{\ast}^{\top}x_t)$.

The above formulation is on disjuctive objective, that is, at a time step the agent stops as soon as she reveals a base arm
	at a position $k$ in the sequence with weight $1$ and she receives reward $\gamma_k$ as the result.
Similarly, we could also consider the case of conjunctive objective. We use Bernoulli random variable $\bw_{t}(a) \in \{0,1\}$ to indicate the {\it weight} of item $a$ at time $t$ satisfying equation (\ref{eq:expectation})). The learning agent observes all items until the first one with weight $0$. Suppose we also consider partial reward: if the $k$-th item is the first item with weight $0$, then the learning agent receives reward $\gamma_k'$; if all items have weight $1$, the learning agent receives reward $1$. The more items the learning agent reveals, the more reward the agent should receive. It is reasonable to assume that
$$
0 = \gamma'_1 \leq \gamma'_2 \leq \cdots \leq \gamma'_K \leq 1.
$$
At the end of time $t$, the learning agent observes $\bO_t, \bw_t(\ba_k^t), k \leq \bO_t$ and receives reward
\wei{For the following, why using $\gamma_{\bO_t - 1}'$ and $\gamma_{k - 1}'$ instead of
	$\gamma_{\bO_t}'$ and $\gamma_{k}'$. It is inconsistent with the above description, and I think the latter makes sense.}
\begin{align*}
\br_{\wedge, t} &= \begin{cases}
\gamma_{\bO_t - 1}'  &\text{if } \bw_t(\ba_{\bO_t}^t) = 0,\\
1 &\text{if } \bw_t(\ba_{\bO_t}^t) = 1,
\end{cases}\\
&=\begin{cases}
\gamma_{k - 1}'  &\text{if } \bw_t(\ba_{i}^t) = 1, i < k, \bw_t(\ba_{k}^t) = 0,\\
1 &\text{if } \bw_t(\ba_{i}^t) = 1, i\leq \abs{\bA_t}.
\end{cases}
\end{align*}

If we define a function $f_{\wedge}$ on $A = (a_1, \ldots, a_{\abs{A}}) \in \cS, w = (w(1), \ldots, w(L))$ by
\begin{align*}
&f_{\wedge} : \cS \times [0,1]^E \to [0,1]\\
&f_{\wedge}(A,w) = \sum_{k = 1}^{\abs{A}} \gamma_k' (\prod_{i = 1}^{k - 1} w(a_i))(1 - w(a_k)) + \prod_{i=1}^{\abs{A}}w(a_i),
\end{align*}
then we have $\br_{\wedge, t} = f_{\wedge}(\bA_t, \bw_t)$ and $\EE[\br_{\wedge, t}] = f_{\wedge}(\bA_t, \theta_{\ast}^{\top}x_t)$ where $x_t = (x_{t,1}, \cdots, x_{t,L}) \in \RR^{d \times L}$. To simplify notations, we assume 
$$
\gamma_k' = 1 - \gamma_k.
$$
Then it holds that
\begin{equation}
\label{eq:ConDisRelation}
f_{\wedge}(A, w) = 1 - f(A, 1 - w).
\end{equation}
Let 
\begin{align*}
A_{\wedge, t}^{\ast} &= \argmax_{A\in \cS} f(A,\theta_{\ast}^{\top}x_t),\\
f_{\wedge, t}^{\ast} &= f(A_t^{\ast}, \theta_{\ast}^{\top}x_t).
\end{align*}
For the conjunctive objective, the goal of our learning algorithm is to minimize the expected cumulative regret in $T$ steps
$$
R_{\wedge}(T) = \EE[\sum_{t=1}^T R_{\wedge}(t, \bA_t)].
$$
where $R_{\wedge}(t, \bA_t) = f_{\wedge, t}^{\ast} - f_{\wedge}(\bA_t, \theta_{\ast}^{\top}x_t)$.



\section{Algorithms and Results}

\subsection{Algorithm}
	
Let $\cH_t$ denote all history at the end of time $t$; $\cH_t$ consists of $\{x_s, \bA_s = (\ba_{1}^s,...,\ba_{\abs{\bA_s}}^s), \bO_s, \bw_s(\ba_k^s), k \in[\bO_s], s\in[t] \}$ and define $\EE_t[\cdot] = \EE[\cdot | \cH_t]$. Here we abuse the notation $\abs{\bA_s}$ to denote the number of elements in the list $\bA_s$. By equation (\ref{eq:expectation}), we have 
$$
\EE[(\gamma_k \bw_{s,\ba_k^s}) | \cH_{s-1}] = \theta_*^{\top} (\gamma_k x_{s,\ba_k^s}).
$$
Using the ridge regression of data 
$$
\{(\gamma_k X_{s,\ba_k^s}, \gamma_k \bw_{s,\ba_k^s})\}_{k \in[\bO_s], s\in[t]},
$$
we get a $l^2$-regularized least-squares estimate of $\theta_*$ with regularization parameter $\lambda > 0$:
\begin{equation}
\hat{\theta}_t = (\bX_t^{\top}\bX_t + \lambda I)^{-1} \bX_t^{\top} \bY_t,
\end{equation}
where $\bX_t \in \RR^{(\sum_{s=1}^{t}\bO_s) \times d}$ is the matrix whose rows are $\gamma_k x_{s,\ba_k^s}^{\top}$ and $\bY_t$ is the column vector whose elements are $\gamma_k \bw_s(\ba_k^s)$, $k \in[\bO_s], s\in[t]$. Let
$$
\bV_t = \bX_t^{\top} \bX_t + \lambda I = \lambda I + \sum_{s=1}^{t} \sum_{k=1}^{\bO_s} \gamma_k^2 x_{s,\ba_k^s}x_{s,\ba_k^s}^{\top}.
$$
Then $\bV_t \in \RR^{d \times d}$ is a positive invertible matrix. The above notifications hold for both disjunctive and conjunctive objectives.

Next we build on a good estimate of differences between $\hat{\theta}_t$ and $\theta_*$ by Theorem 2 in \cite{abbasi2011improved}, which states the following results.
	
\begin{theorem}[\cite{abbasi2011improved}]
\label{thm:theta_estimate}
Let 
$$
\beta_{t}(\delta) = \sqrt{\log(\det(\bV_{t})) + 2 \log(\frac{1}{\delta})} + \sqrt{\lambda}.
$$
Then for any $\delta > 0$, with probability at least $1 - \delta$, for all $t > 0$, we have
\begin{equation}
\label{eq:estimateTheta}
\norm{\hat{\theta}_t - \theta_{\ast}}_{\bV_{t}} \leq \beta_{t}(\delta).
\end{equation}
\end{theorem}

This theorem states that with high probability, the estimate $\hat{\theta}$ lies in the ellipsoid centered at $\theta_*$  with confidence radius $\beta_t(\delta)$ under $\bV_t$ norm. Building on this, we can define an upper confidence bound of the expected weight of item $a$ by
\begin{equation}
\bU_t(a) = \min\{\hat{\theta}_{t-1}^{\top}x_{t,a} + \beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}, 1\}.
\end{equation}

The fact that $\bU_t(a)$ is an upper confidence bound of $\theta_*^{\top}x_{t,a}$ is proved in the following lemma.
\begin{lemma}
\label{lem:estimateU}
When (\ref{eq:estimateTheta}) holds for time $t$, we have
$$
0 \leq \bU_t(a) - \theta_{\ast}^{\top}x_{t,a} \leq 2\beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}.
$$
\end{lemma}
\begin{proof}
\begin{align*}
\abs{\hat{\theta}_{t-1}^{\top}x_{t,a} - \theta_{\ast}^{\top}x_{t,a}} &\leq \norm{\hat{\theta}_{t-1} - \theta_{\ast}}_{\bV_{t-1}} \norm{x_{t,a}}_{\bV_{t-1}^{-1}} \\
&\leq \beta_{t}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}.
\end{align*}
\end{proof}

Our proposed algorithm, ConComCascade, is described in Algorithm \ref{alg:ConComCascade}. First, it computes the upper confidence bounds (UCBs) $\bU_t \in [0,1]^{E}$ on the expected weights of all items in $E$. Second, it recommends the output $\bA_t$ by the offline oracle with estimated weights $\bU_t$. Third, play $\bA_t$ and observes all feedback of items until first $0$ weight come out; the learning agent observes $\bO_t$ items and $\bw_t(\ba_k^t), k \in [\bO_t]$, where 
$$
\bw_t(\ba_{k}^t) = \begin{cases} 0, ~~k < \bO_t\\ 1, ~~k = \bO_t\end{cases}
$$ 
or
$$
\bw_t(\ba_k^t) = 1, k \leq \bO_t = \abs{\bA_t}.
$$
Then, the learning agent update $\bV_t, \bX_t, \bY_t$ in order to get a newer estimate $\hat{\theta}_t$ of $\theta_*$ and new confidence radius $\beta_t(\delta)$. In this part, we use the notation $[A; B]$ to denote a matrix $\begin{pmatrix} A\\ B\end{pmatrix}$.

\begin{algorithm}
\caption{ConComCascade}
\label{alg:ConComCascade}
\begin{algorithmic}[1]
\STATE {//Initialization}
\STATE {Parameters: $\delta > 0, \lambda \geq C_\gamma$}
\STATE {Parameters: $1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_K \geq 0$}
\STATE {$\hat{\theta}_0 = 0, \beta_0(\delta) = 1, \bV_0 = \lambda I$}
\STATE{}

\FORALL {$t=1,2,\ldots$}
\STATE {Obtain context $x_{t,a}$ for all $a\in E$}
\STATE{}

\STATE {//Compute UCBs}
\STATE {$\forall a\in E: $}
\STATE {$\bU_t(a) \leftarrow \min\{\hat{\theta}_{t-1}^{\top}x_{t,a} + \beta_{t-1}(\delta)\norm{x_{t,a}}_{\bV_{t-1}^{-1}}, 1\}$}
\STATE {}
		
\STATE {//Choose action $\bA_t$ using UCBs $\bU_t$}
\STATE {$\bA_t=(\ba_1^t,...,\ba_{\abs{\bA_t}}^t) \leftarrow \argmax_{A \in \cS} f(A, \bU_t)$}
\STATE {Play $\bA_t$ and observe $\bO_t, \bw_t(\ba_k^t), k\in[\bO_t]$}
\STATE {}

\STATE {//Update statistics}
\STATE {$\bV_{t} \leftarrow \bV_{t-1} + \sum_{k=1}^{\bO_t} \gamma_k^2 x_{t, \ba_k^t}x_{t, \ba_k^t}^{\top}$}
\STATE {$\bX_t \leftarrow [\bX_{t-1}; ~\gamma_1' x_{t, \ba_1^t}^{\top};  ~\ldots; ~\gamma_{\bO_t}' x_{t, \ba_{\bO_t}^t}^{\top}]$}
\STATE {$\bY_t \leftarrow [\bY_{t-1}; ~\gamma_1' \bw(\ba_1^t); ~\ldots; ~\gamma_{\bO_t}' \bw_t(\ba_{\bO_t}^t)]$}
\STATE {$\hat{\theta}_t \leftarrow (\bX_t^\top \bX_t + \lambda I)^{-1} \bX_{t}^\top \bY_t$}
\STATE {$\beta_{t}(\delta) \leftarrow \sqrt{\log(\det(\bV_{t})) + 2 \log(\frac{1}{\delta})} + \sqrt{\lambda}$}
			
\ENDFOR {~~$t$}
\end{algorithmic}
\end{algorithm}
	

\subsection{Results}

Here is our main theorem.
\begin{theorem}
\label{thm:main}
Given partial reward parameters $0 = \gamma_0 \leq \gamma_1 \leq \cdots \gamma_{K-1} \leq c f^*$ and regularization parameter $\lambda \geq C_\gamma$ for some constant $\frac{1}{2} < c < 1$, for any $\delta > 0$, with probability at least $1 - \delta$, the cumulative expected regret of our algorithm, {\it ConComCascade}, satisfies
\begin{equation}
R(T) = O(d^{3/4}\log^{3/4}(C_\gamma T)\sqrt{KT}),
\end{equation}
where
\begin{align*}
f^* &= \min_{1 \leq t \leq T} f_t^*\\
C_\gamma &= \sum_{k=1}^{K} \gamma_k'^2 = \sum_{k=1}^{K-1} (1 - \gamma_k)^2 = O(K).
\end{align*}
\end{theorem}

Suppose the reward function is disjunctive. At every time $t$, the forecaster recommends a list $\bA_t$. Then the user checks from the first item to the end. The weight $\bw_t(a)$ denote whether the user click on item $a$ or not. The user stops if she clicks on one item of $\bA_t$ or she has checked all items of $\bA_t$. If the user clicks on the first item, then the learning agent receives reward $1$. If the user clicks on the $k$-th reward of $\bA_t$, then the reward is $\gamma_k$. If the user doesn't click on any item, then the reward is $0$. Here we further assume $\gamma_2 \geq \gamma_3 \geq \ldots \geq \gamma_K$. If we define a function $f_{\vee}$ on $A = (a_1, \ldots, a_{\abs{A}}) \in \cS$ and $w = (w(1), \ldots, w(L))$ by
\begin{align*}
&f_{\vee} : \cS \times [0,1]^E \to [0,1]\\
&f_{\vee}(A,w) = \sum_{k=1}^{\abs{A}}\gamma_k (\prod_{i=1}^{k-1} (1 - w(a_i)) w(a_k).
\end{align*}
Here we use notation that $\gamma_1 = 1$. Then the expected reward of $\bA_t$ is $f_{\vee}(\bA_t, \theta_*^{\top}x_t)$.

Then we have a relationship between $f$ and $f_{\vee}$
\begin{equation}
\label{eq:relation}
\begin{split}
&f_{\vee}(A, w; 1, \gamma_2, \ldots, \gamma_K) \\
&~~= 1 - f(A, 1 - w; 0, 1-\gamma_2, \ldots, 1-\gamma_K)
\end{split}
\end{equation}
If $\gamma$ is decreasing, then $1 - \gamma$ is increasing. Thus we have $1 - \gamma$ a feasible parameters for $f$. Then we could have a similar result for disjunctive case.
\begin{theorem}
Disjunctive case
\end{theorem}

To prove theorem \ref{thm:main}, we could start from estimating $\EE[\Delta_{t, \bA_t}|\cH_t]$ for every time $t$. In fact, we can derive the following lemma.
\begin{lemma}
\label{lem:DeltaEstimate}
For any time $t$ and $A = (a_1, \ldots, a_{\abs{A}})$, if $f(A_t^*, \bU_t) \leq f(A, \bU_t)$, then we have
$$
\Delta_{t,A} \leq \sum_{k=1}^{\abs{A}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t,a_k}}_{\bV_{t-1}^{-1}};
$$
if $f_{\wedge}(A_t^*, \bU_t) \leq f_{\wedge}(A, \bU_t)$, then we have
$$
\Delta_{t,A} \leq \sum_{k=1}^{\abs{A}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t,a_k}}_{\bV_{t-1}^{-1}}.
$$
\end{lemma}

So if the chosen action $\bA_t$ is not suboptimal, which means
$$
f(A_t^*, \bU_t) \leq f(\bA_t, \bU_t).
$$
Then we can derive an estimate of $\Delta_{t, \bA_t}$, however, in terms of all elements in $\bA_t$. 

However, this estimate involves all elements of $\bA_t$. Only when the learning agent has observed all elements of $\bA_t$, we can use such an estimate, which means we are able to estimate
$$
\EE[\Delta_{t, \bA_t}] = \EE[\Delta_{t, \bA_t}(1/p_{\bA_t}) \bOne\{\Delta_{t, \bA_t} > 0, \bO_t \geq \abs{\bA_t}\}].
$$
Problem might happen if $p_{\bA_t}$ is small. Thus we need the following lemma to find a good substitution of $\bA_t$.

\begin{lemma}
\label{lem:prefixExist}
Suppose the weights for items in $E$ are $w = (w_a)_{a \in E}$. Let $A = (a_1, ..., a_{\abs{A}})$ be a list and let $p_A$ denote the probability that all of items in $A$ are observed; equivalently, $p_A = \prod_{k=1}^{\abs{A}-1}w(a_i)$. Let $B_k = (a_1, ..., a_k), k \leq \abs{A}$ be a prefix of $A$. Then for any given constant $\frac{1}{2} < c < 1$, there exists a prefix $B$ of $A$ such that
$$
p_B \geq c f^* - \gamma_{\abs{B}}, \qquad \Delta_B \geq \frac{1}{2} \Delta_A,
$$
where
$$
f^* = \max_{A'} f(A', w), \qquad \Delta_{A'} = f^* - f(A', w).
$$
\end{lemma}

From this lemma, we can find a prefix $\bB_t$ of $\bA_t$ such that $p_{B_t}$ is large enough and $\Delta_{\bB_t}$ will not be too small. Then follow the above deductions, we can get a good estimate for $\EE[\Delta_{t, \bA_t}|\cH_{t-1}]$ in terms of $\norm{x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}$. Once we have estimated $\norm{x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}$ and $\beta_{t-1}(\delta)$, we can get an estimate for $\EE[\Delta_{t, \bA_t}|\cH_{t-1}]$, thus for $R(T)$. We leave all details in the next subsection.


\subsection{Proofs}

\begin{lemma}
\label{lem:increasing} 
Given $1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_K \geq 0$, $f(A, w)$ is increasing with respect to $w$; equivalently, suppose $0 \leq w \leq w' \leq 1$ as column vectors in $\RR^L$, we have for any $A \in \prod^{\leq K}(E)$,
$$
f(A, w) \leq f(A, w').
$$
By relation (\ref{eq:ConDisRelation}), it also holds that
$$
f_{\wedge}(A, w) \leq f_{\wedge}(A, w').
$$
\end{lemma}
\begin{proof}
It suffices to prove when $A = (1, \ldots, m)$, where $1 \leq m \leq K$. Because for $1 \leq k \leq m$,
\begin{align*}
&\gamma_{k} - \sum_{i=k+1}^m \gamma_i \prod_{j = k + 1}^{i - 1} (1 - w_j') w_i'\\
\geq &~\gamma_k [1 - \sum_{i=k+1}^m \prod_{j=k+1}^{i-1}(1 - w_j') w_i']\\
\geq &~\gamma_{k-1} \cdot 0 = 0,
\end{align*}
then
\begin{align*}
&\gamma_k w_k + (1 - w_k)\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i'\\
\leq &\gamma_k w_k' + (1 - w_k')\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i'\\
\end{align*}
Therefore, 
\begin{align*}
& f(A; w_1, \dots, w_k, w_{k+1}', \dots, w_m')\\
&=\sum_{i=1}^{k-1} \gamma_i \prod_{j=1}^{i-1}(1 - w_j) w_i + \prod_{j=1}^{k-1}(1 - w_j) \\
&\qquad \cdot [\gamma_k w_k + (1 - w_k)\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i']\\
&\leq \sum_{i=1}^{k-1} \gamma_i \prod_{j=1}^{i-1}(1 - w_j) w_i + \prod_{j=1}^{k-1}(1 - w_j) \\
&\qquad \cdot [\gamma_k w_k' + (1 - w_k')\sum_{i=k+1}^m \gamma_i \prod_{j=k+1}^{i-1}(1 - w_j') w_i']\\
&=f(A; w_1, \ldots, w_{k-1}, w_{k}', \ldots, w_m').
\end{align*}
\end{proof}

\begin{lemma}
\label{lem:estimateTech}
Suppose $0 \leq p_1, \ldots, p_m \leq 1$ and $u_1, \ldots, u_m \geq 0$. Let $w_i = \min\{p_i + u_i, 1\}, 1 \leq i \leq m$. Then
\begin{align*}
f(A^{(m)}, w) &\leq f(A^{(m)}, p) + \sum_{k=1}^{m} \gamma_k u_k,\\
f_{\wedge}(A^{(m)}, w) &\leq f_{\wedge}(A^{(m)}, p) + \sum_{k=1}^{m} \gamma_k u_k,
\end{align*}
for any positive $\gamma_k, \gamma_k'$ with $\gamma_k' = 1 - \gamma_k$, where $A^{(m)} = (1, 2, \ldots, m), w = (w_1, \ldots, w_m)$ and $p = (p_1, \ldots, p_m)$. Notice that the parameters involved in the conclusions are $\gamma_k, k \in [m]$ while the parameters used in the definition of $f_{\wedge}$ are $\gamma_k', k \in [m]$.
\end{lemma}
\begin{proof}
We prove this by induction. It holds obviously for $m = 1$.
\begin{align*}
&~~f(A^{(m+1)}, w)\\
&= f(A^{(m)}, w) + \gamma_{m+1}\prod_{k=1}^m(1 - w_k) w_{m+1}\\
&\leq f(A^{(m)}, w) +  \gamma_{m+1} \prod_{k=1}^m(1 - p_k) (p_{m+1} + u_{m+1})\\
&\leq f(A^{(m)}, p) + \sum_{k=1}^m \gamma_k u_k \\
&\qquad + \gamma_{m+1} \prod_{k=1}^m(1 - p_k) p_{m+1} + \gamma_{m+1} u_{m+1}\\
&= f(A^{(m+1)}, p) + \sum_{k=1}^{m+1} \gamma_k u_k
\end{align*}
\begin{align*}
&~~f_{\wedge}(A^{(m+1)}, w) \\
&= f_{\wedge}(A^{(m)}, w) -\prod_{k=1}^{m} w_k \\
&\qquad+ \gamma_{m+1}' (\prod_{k=1}^{K} w_k) (1 - w_{K+1})+ \prod_{k=1}^{K+1} w_k\\
&= f_{\wedge}(A^{(m)}, w) - (1 - \gamma_{m+1}') (\prod_{k=1}^{m} w_k) (1 - w_{m+1})\\
&\leq f_{\wedge}(A^{(m)}, w) - (1 - \gamma_{m+1}') (\prod_{k=1}^{m} p_k) (1 - w_{m+1})\\
&\leq f_{\wedge}(A^{(m)}, w) -(1 - \gamma_{m+1}') (\prod_{k=1}^{m} p_k)  (1 - p_{m+1} - u_{m+1})\\
&\leq f_{\wedge}(A^{(m)}, p) +  \sum_{k=1}^{m} \gamma_k u_k \\
&\qquad - (1 - \gamma_{m+1}') (1 - p_{m+1}) \prod_{k=1}^{m} p_k + (1 - \gamma_{m+1}') u_{m+1}\\
&\leq f_{\wedge}(A^{(m+1)}, p) + \sum_{k=1}^{m+1} \gamma_k u_k
\end{align*}
\end{proof}

\begin{proof}[ of lemma \ref{lem:DeltaEstimate}]
Using lemma \ref{lem:increasing} and lemma \ref{lem:estimateU}, we can deduce 
$$
f_t^{\ast} = f(A_t^{\ast}, \theta_{\ast}^{\top}x_t) \leq f(A_t^{\ast}, \bU_t).
$$
Using lemma \ref{lem:estimateU} and lemma \ref{lem:estimateTech}, we can deduce
$$
f(\bA_t, \bU_t) \leq f(\bA_t, \theta_{\ast}^{\top}x_t) + \sum_{k=1}^{\abs{\bA_t}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t, \ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
Combining the above two inequalities with $f(A_t^{\ast}, \bU_t) \leq f(\bA_t, \bU_t)$, we can conclude that
$$
\Delta_{t, \bA_t} = f_t^{\ast} - f(\bA_t, \theta_{\ast}^{\top}x_t) \leq \sum_{k=1}^{\abs{\bA_t}} \gamma_k \beta_{t-1}(\delta)\norm{x_{t, \ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
The result for the disjunctive objective is similar.
\end{proof}

\begin{lemma}
\label{lem:prefixRelation}
Suppose $A = (a_1, ..., a_{\abs{A}})$. For the conjunctive objective, it holds that
$$
p_A \geq 1 - \frac{f(A, w)}{\gamma_{\abs{A}}}.
$$
Let $B_k = (a_1, ..., a_k), k \leq \abs{A}$ be a prefix of $A$. Then for the disjunctive objective, we have the following properties.
\begin{itemize}
\item[(1)] $f_{\wedge}(A, w) \leq \gamma_{\abs{A}}' + (1 - \gamma_{\abs{A}}') p_{\wedge, A}$;
\item[(2)] $f_{\wedge}(B_{k+1}, w) \leq f_{\wedge}(B_k, w)$;
\item[(3)] $p_{\wedge, B_{k+1}} \leq p_{\wedge, B_k}$;
\item[(4)] $f_{\wedge}(B_k, w) \leq \gamma_{k+1}' + (1 - \gamma_{k+1}') p_{B_{k+1}}, ~k < \abs{A}$,
\end{itemize}
where $1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_K \geq 0$ and $0 = \gamma_1' \leq \gamma_2' \leq \cdots \leq \gamma_K' \leq 1$.
\end{lemma}

\begin{proof}
It suffices to prove these properties when $A = A^{(m)}$ and $w = (w_1, \ldots, w_m)$. First, for the conjunctive objective,
$$
f(A, w) \geq \gamma_{m} (1 - p_{A} + p_{A} w_{m}) \geq \gamma_{m} (1 - p_{A}).
$$
Then for the disjunctive objective,
\begin{itemize}
\item[(1)]
\begin{align*}
f_{\wedge}(A, w) &\leq \gamma_m'(1 - p_{\wedge, A} w_m) + p_{\wedge, A} w_m \\
&\leq \gamma_m' + (1 - \gamma_m') p_{\wedge, A};
\end{align*}


\item[(2)]
\begin{align*}
&f(B_k, w) - f(B_{k+1}, w)\\
&=\prod_{i=1}^{k}w_i - \gamma_{k+1}' (\prod_{i=1}^{k}w_i) (1 - w_{k+1}) - (\prod_{i=1}^{k}w_i) w_{k+1}\\
&=(1 - \gamma_{k+1}') (\prod_{i=1}^{k}w_i) (1 - w_{k+1}) \geq 0;
\end{align*}

\item[(3)]
obvious;

\item[(4)]
\begin{align*}
f_{\wedge}(B_k, w) &\leq \gamma_{k}' (1 - p_{B_{k+1}}) + p_{B_{k+1}}\\
&\leq \gamma_{k+1}' (1 - p_{B_{k+1}}) + p_{B_{k+1}} \\
&= \gamma_{k+1}' + (1 - \gamma_{k+1}') p_{B_{k+1}}
\end{align*}
\end{itemize}
\end{proof}

\begin{lemma}
Suppose $A = (a_1, ..., a_{\abs{A}})$. For the time $t$ and the disjunctive objective, there exists a prefix $B$ of $A$ such that 
$$
p_{B} \geq \frac{1}{2}f_{\wedge, t}^{\ast} - \gamma_{\abs{B}}', \qquad \Delta_{\wedge, t, B} \geq \frac{1}{2}\Delta_{\wedge, t, A}.
$$ 
\end{lemma}
\begin{proof}
Notice that $f_{\wedge, t}^{\ast} \leq 1$. If $p_{A} \geq \frac{1}{2}f_{\wedge, t}^{\ast} - \gamma_{\abs{A}}'$, then the lemma is proved. 

Suppose this does not hold. By lemma \ref{lem:prefixRelation}, we have $[f_{\wedge}(B_k,w), \gamma_k' + (1 - \gamma_k')p_{B_k}]$ is an interval and the intersection between interval $[f_{\wedge}(B_k,w), \gamma_k' + (1 - \gamma_k')p_{B_k}]$ and interval $[f_{\wedge}(B_{k+1},w), \gamma_{k+1}' + (1 - \gamma_{k+1}')p_{B_{k+1}}]$ in not empty. Notice when $k=\abs{A}$, $f_{\wedge}(B_k,w) = f_{\wedge}(A, w)$ and when $k = 1$, $\gamma_k' + (1 - \gamma_k')p_{B_k} = 1$. So we have
$$
[f_{\wedge}(A,w), 1] = \bigcup_{k=1}^{\abs{A}} [f_{\wedge}(B_k,w), \gamma_k' + (1 - \gamma_k')p_{B_k}].
$$
Then there exists a $k$ such that $f(B_k,w) \leq \frac{1}{2}f_{\wedge, t}^{\ast} \leq \gamma_k' + (1 - \gamma_k')p_{B_k}$. Therefore, the results are derived.
\end{proof}


\begin{proposition}
Suppose the equation (\ref{eq:estimateTheta}) holds for any time $t$ and $1 = \gamma_1 \geq \gamma_2 \geq \cdots \geq \gamma_K \geq 0, \gamma_k' = 1 - \gamma_k$. If we have
$$
\gamma_K \geq (1+\frac{1}{n_0})f^{\ast}, \qquad \gamma_K' \geq \frac{1}{4} f_{\wedge}^{\ast}
$$
for some positive integer $n_0$, then
\begin{align*}
&E[\Delta_{t, \bA_t}|\cH_t] \leq (n_0+1) \beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}},\\
&E[\Delta_{\wedge, t, \bA_t}|\cH_t] \leq \frac{8}{f_{\wedge}^{\ast}} \beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}},
\end{align*}
where $f^{\ast} = \max_{t} f_t^{\ast}, f_{\wedge}^{\ast} = \max_{t} f_{\wedge, t}^{\ast}$.
\end{proposition}
\begin{proof}
The conjunctive case can be derived directly. For the disjunctive case, there exists a prefix $\bB_t$ of $\bA_t$ such that $p_{\bB_t} \geq \frac{1}{4}f_t^*$ and $\Delta_{\bB_t} \geq \frac{1}{2}\Delta_A$. Then we have
$$
\EE[\Delta_{\wedge, t, \bA_t} | \cH_{t-1}] \leq \frac{8}{f_t^*} \EE[\Delta_{\wedge, t, \bB_t} \bOne\{\Delta_{\wedge, t, \bB_t} > 0, \bO_t \geq \abs{\bB_t}\}
$$
Using $f(\bA_t^*,\bU_t) \leq f(\bA_t,\bU_t) \leq f(\bB_t,\bU_t)$, lemma \ref{lem:estimateU} and lemma \ref{lem:increasing}, it holds that
$$
f(\bA_t^*, \theta_*^{\top}X_t) \leq f(\bB_t,\bU_t).
$$
Then by lemma \ref{lem:estimateTech}
$$
f(\bB_t,\bU_t) \leq f(\bB_t, \theta_*^{\top}x_t) + \sum_{k=1}^{\abs{\bB_t}}\gamma_k'\beta_{t-1}(\delta)\norm{x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
Therefore,
$$
\Delta_{\bB_t} \leq \sum_{k=1}^{\abs{\bB_t}}\beta_{t-1}(\delta)\norm{\gamma_k' x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
\end{proof}
	
\begin{lemma} % det(V_t)
$\det(\bV_t) \leq (\lambda + C_\gamma t/d)^d.$
\end{lemma}
\begin{proof}
We have $\det(\bV_t) \leq (\trace(\bV_t)/d)^d$ and
\begin{align*}
&\trace(\bV_t)\\
& = \trace(\lambda I) + \sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s,\abs{\bA_s}\}} \gamma_k'^2 \trace(x_{s,\ba_k^s} x_{s,\ba_k^s}^{\top})\\	
& = d \lambda + \sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s,\abs{\bA_s}\}} \gamma_k'^2 \norm{x_{s,\ba_k^s}}_2^2\\
& \leq d \lambda + \sum_{s=1}^t\sum_{k=1}^{K}\gamma_k'^2=d\lambda + tC_\gamma.
\end{align*}
\end{proof}

\begin{lemma} % technical lemma for det
Let $x_i \in \RR^{d \times 1}, 1 \leq i \leq n$. Then we have
$$
\det(I + \sum_{i=1}^n x_i x_i^{\top}) \geq 1 + \sum_{i=1}^n \norm{x_i}_2^2.
$$
\end{lemma}
\begin{proof}
Denote the eigenvalues of $I + \sum_{i=1}^n x_i x_i^{\top}$ by $1+\alpha_1,...,1+\alpha_d$ with $\alpha_j \geq 0$, $1\leq j\leq d$. Then
\begin{align*}
&\det(I + \sum_{i=1}^n x_i x_i^{\top})\\
&= \prod_{j=1}^d (1 + \alpha_j)\geq 1 +\sum_{j=1}^d \alpha_j =1-d + \sum_{i=1}^d (1+\alpha_i) \\
&=1-d + \trace(I + \sum_{i=1}^n x_i x_i^{\top})= 1-d + d + \sum_{i=1}^n \norm{x_i}_2^2\\
&=1 + \sum_{i=1}^n \norm{x_i}_2^2.
\end{align*}
\end{proof}

\begin{lemma}
If $\lambda \geq C_\gamma$, it holds that
$$
\sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s, \abs{\bA_s}\}} \norm{\gamma_k' x_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \leq 2\log(\det(\bV_t)).
$$
\end{lemma}
\begin{proof}
\begin{align*}
&\det(\bV_t) = \det(\bV_{t-1} + \sum_{k=1}^{\bO_t} (\gamma_k' X_{t,\ba_k^{t}})(\gamma_k' X_{t, \ba_k^{t}}^{\top}))\\
&=\det(\bV_{t-1})\\
&~~~~\det(I + \sum_{k=1}^{\bO_t} \gamma_k'\bV_{t-1}^{-1/2}x_{t,\ba_{k}^{t}} (\gamma_k' \bV_{t-1}^{-1/2}x_{t,\ba_{k}^{t}})^{\top})\\
&\geq \det(\bV_{t-1}) (1 + \sum_{k=1}^{\bO_t} \norm{\gamma^{k-1}x_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2)\\
&\geq \det(\lambda I)\prod_{s=1}^{t}(1 + \sum_{k=1}^{\bO_s} \norm{\gamma^{k-1}x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)
\end{align*}
Because
$$
\norm{\gamma_k' x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2 \leq \norm{\gamma_k' x_{s,\ba_k^s}}_2^2/\lambda_{\min}(\bV_{s-1}) \leq \gamma_k'^2 /\lambda,
$$
we have 
$$
\sum_{k=1}^{\bO_s} \norm{\gamma_k' x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2 \leq \frac{1}{\lambda} \sum_{k=1}^{K} \gamma_k'^2 = C_\gamma /\lambda \leq 1
$$
Using the fact that $ 2\log(1+u) \geq u$ for any $u \in [0,1]$, we get
\begin{align*}
&\sum_{s=1}^t \sum_{k=1}^{\bO_s}\norm{\gamma_k' x_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \\
&\leq 2\sum_{s=1}^t\log(1 + \sum_{k=1}^{\bO_s} \norm{\gamma_k' x_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)\\
&\leq 2(\log(\det(\bV_t)) - \log(\det(\lambda I))) < 2\log(\det(\bV_t))
\end{align*}
\end{proof}

\begin{proof}[ of theorem \ref{thm:main}]
With probability at least $1-\delta$,
\begin{equation}
\begin{split}
&R_T =\sum_{t=1}^{T} \EE_{t}[\Delta_{t, \bA_t}] \\
&\leq \sum_{t=1}^{T} 4 \beta_{t-1}(\delta) \EE[\sum_{k=1}^{\bO_t}\norm{\gamma_k' X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}]\\
&\leq 4 \beta_T(\delta) \sum_{t=1}^{T} \bO_t \EE[\sum_{k=1}^{\bO_t}\norm{\gamma_k'^2 X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]\\
&\leq 4 \beta_T(\delta) \sqrt{(\sum_{t=1}^{T} \bO_t) \EE[\sum_{t=1}^{T} \sum_{k=1}^{\bO_t}\norm{\gamma_k'^2 X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]}  \\
&\leq O(d\log(C_\gamma T)\sqrt{TK})
\end{split}
\end{equation}
\end{proof}
	
	
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}
	
\bibliography{cascade_reference}
\bibliographystyle{icml2015}
	
\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
