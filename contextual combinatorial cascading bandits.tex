%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% my settings
\usepackage{amssymb,amsfonts,amsmath,bbm}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage[all]{xy}
\usepackage{empheq}
\usepackage{bm}
\usepackage{algorithm,algorithmic}
\usepackage{multirow}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bOne}{\mathbbm{1}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\trace}{\mathrm{trace}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\| #1 \|}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}%[chapter]
\newtheorem{definition}[theorem]{Definition}%[chapter]
\newtheorem{corollary}[theorem]{Corollary}%[chapter]
\newtheorem{lemma}[theorem]{Lemma}%[chapter]
\newtheorem{example}[theorem]{Example}
\newenvironment{proof}{\noindent {\textbf{Proof. }}}{$\Box$ \medskip}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Contextual Combinatorial Cascading Bandits}

\begin{document} 
	
\twocolumn[
\icmltitle{Contextual Combinatorial Cascading Bandits}
	
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute, 314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute, 27182 Exp St., Toronto, ON M6H 2T1 CANADA}
	
% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}
	
\vskip 0.3in
]
	
\begin{abstract} 
The purpose of this document is to provide both the basic paper template and submission guidelines.
\end{abstract} 
	
\section{Introduction}
	
\section{Related Works}
	
\section{Contextual Combinatorial Cascading Bandits}
	
\subsection{Setting}
	
We model our problem as a contextual combinatorial cascading bandit. Suppose we have $E=\{1,...,L\}$ a finite set of $L$ ground items. Let $\prod^k=\{(a_1,...,a_k): a_1,...,a_k \in E, a_i \neq a_j \text{ for any } i \neq j\}$ be the set of all $k$-tuples of distinct items from $E$. Let $\cS \subset \prod^{\leq K}(E)$ consist of feasible actions with length no more than $K$.
	
At time $t$, the learning agent is revealed with feature vectors $X_{t,a} \in \RR^d$ for every basic arm $a \in E$, where we have $\norm{X_{t,a}}_2 \leq 1$; this feature vector combines both information of the user and the corresponding basic arm. We use Bernoulli random variable $\bw_{t}(a) \in \{0,1\}$, which is called weight for arm $a$ at time $t$, to indicate whether the user on round $t$ will click on the item $a$ or not. Assume $\bw_{t,a}$ are mutually independent and satisfy
\begin{equation}
\label{eq:expectation}
\EE[\bw_{t,a} | X_{t,a}] = \theta_*^{\top} X_{t,a}
\end{equation}
where $\theta_*$ is an unknown $d$-dimensional vector with the assumption that $\norm{\theta_*}_2 \leq 1$ and $0 < \theta_*^{\top} x_{t,a} < 1$ for all $t, a$. At time $t$, the learning agent chooses a solution $\bA_t=(\ba_{1}^t,...,\ba_{\abs{\bA_t}}^t) \in \cS$ based on its past observations. The user then checks from the first item and stops if she has checked all items or click on one interesting. Suppose we add position discount $0 \leq \gamma_1,...,\gamma_K\leq \gamma_0 \leq 1$. If the $k$-th item of $\bA_t$ appears blocking, the reward we receive is $\gamma_k$. And if the whole path of $\bA_t$ is unblocked, the received reward is $\gamma_0$. At the end of time $t$, the agent observes $\bO_t$ items in $\bA_t$ and receive the reward
\begin{align*}
\br_t &= \min_{1 \leq k \leq \abs{\bA_t}} [\gamma_k (1 - \bw_t(\ba_k^t)) +\gamma_0 \bw_t(\ba_k^t)] \\
&= \bigwedge_{k=1}^{\abs{A_t}} [\gamma_k (1 - \bw_t(\ba_k^t)) +\gamma_0 \bw_t(\ba_k^t)],
\end{align*}
where we use the notation that $\bigwedge_{1\leq i\leq n}a_i = \min_{1\leq i\leq n}a_i$. Note that every time $t$, we have observed $\bw_t(\ba_k^t), 1\leq k\leq\bO_t$. 
	
If we define a function $f$ on $A=(a_1,...,a_{\abs{A}}) \in \cS, w=(w(1),...,w(L))$ by
\begin{align*}
f : \cS \times [0,1]^E \to [0,1]&~\\
f(A,w;\gamma_1,...,\gamma_K,\gamma_0) =& \sum_{k=1}^{\abs{A}}\gamma_k (\prod_{i=1}^{k-1}w(a_i))(1 - w(a_k)) \\
\qquad \qquad &~+ \gamma_0 \prod_{i=1}^{\abs{A}}w(a_i),
\end{align*}
then we have $\br_t = f(\bA_t, \bw_t)$ and $\EE[\br_t]=f(\bA_t,\theta_*^{\top}X_t)$ where $X_t=(X_{t,1} \cdots X_{t,L})\in\RR^{d\times L}$. Let 
$$
A_t^* = \argmax_{A\in \cS} f(A,\theta_*^{\top}X_t).
$$ 
The goal of our learning algorithm is to minimize the expected cumulative regret in $T$ steps
$$
R(T) = \EE[\sum_{t=1}^T f(A_t^*, \theta_*^{\top}X_t) - f(\bA_t, \theta_*^{\top}X_t)].
$$
In the rest of this paper, we denote $\Delta_{t,\bA_t} = f(A_t^*, \theta_*^{\top}X_t) - f(\bA_t, \theta_*^{\top}X_t)$ and $R(T) = \EE[\sum_{t=1}^T \Delta_{t,\bA_t}]$.
	
Let $\cH_t$ denote all history at the end of time $t$; $\cH_t$ consists of $\{X_s, \bA_s=(\ba_{1}^s,...,\ba_{\abs{\bA_s}}^s), \bO_s, \bw_s(\ba_k^s): k \in[\bO_s], s\in[t] \}$ and $\EE_t[\cdot] = \EE[\cdot | \cH_t]$. By equation (\ref{eq:expectation}), we have $\EE[(\gamma_k'\bw_{s,\ba_k^s}) | \cH_{s-1}] = \theta_*^{\top} (\gamma_k' X_{s,\ba_k^s})$. By ridge regression of data 
$$
\{(\gamma_k' X_{s,\ba_k^s}, \gamma_k'\bw_{s,\ba_k^s})\}_{k \in[\bO_s], s\in[t]}
$$
where $\gamma_k' = \begin{cases}
\gamma_0, ~~k=1\\
\gamma_0-\gamma_k, ~~k\geq 2,
\end{cases}$
let $\hat{\theta}_t$ be the $l^2$-regularized least-squares estimate of $\theta_*$ with regularization parameter $\lambda > 0$:
\begin{equation}
\hat{\theta}_t = (X^{t,\top}X^{t} + \lambda I)^{-1} X^{t, \top} \bY^t
\end{equation}
where $X^t \in \RR^{(\sum_{s=1}^{t}\bO_s) \times d}$ is the matrix whose rows are $\gamma_k' X_{s,\ba_k^s}^{\top}$ and $\bY^t$ is the column vector whose elements are $\gamma_k' \bw_s(\ba_k^s)$, $k \in[\bO_s], s\in[t]$. Let
$$
\bV_t = X^{t,\top}X^{t} + \lambda I = \lambda I + \sum_{s=1}^{t} \sum_{k=1}^{\bO_s} \gamma_k'^2 X_{s,\ba_k^s}X_{s,\ba_k^s}^{\top}.
$$
Then $\bV_t \in \RR^{d \times d}$ is a positive invertible matrix.


\subsection{Algorithm}
	
\begin{theorem}[Theorem 2 in \cite{abbasi2011improved}]
\label{thm:theta_estimate}
Let 
$$
\beta_{t}(\delta) = \sqrt{\log(\det(\bV_{t})) + 2 \log(\frac{1}{\delta})} + \sqrt{\lambda}.
$$
Then for any $\delta > 0$, with probability at least $1 - \delta$, for all $t > 0$, we have
\begin{equation}
\norm{\hat{\theta}_t - \theta_*}_{\bV_{t}} \leq \beta_{t}(\delta).
\end{equation}
\end{theorem}
	
Our proposed algorithm, ConComCascade, is described in Algorithm \ref{alg:ConComCascade}. First, it computes the upper confidence bounds (UCBs) $\bU_t \in [0,1]^{E}$ on the expected weights of all items in $E$. The UCB of item $a$ at time $t$ is defined as:
\begin{equation}
\bU_t(a) = \min\{\hat{\theta}_{t-1}^{\top}X_{t,a} + \beta_{t-1}(\delta)\norm{X_{t,a}}_{\bV_{t-1}^{-1}}, 1\}.
\end{equation}
By theorem \ref{thm:theta_estimate}, it holds that
\begin{lemma}
\label{lem:estimateU}
For any $\delta > 0$, with high probability at least $1 - \delta$, for any $t>0$,
$$
0 \leq \bU_t(a) - \theta_*^{\top}X_{t,a} \leq 2\beta_{t-1}(\delta)\norm{X_{t,a}}_{\bV_{t-1}^{-1}}.
$$
\end{lemma}
\begin{proof}
\begin{align*}
\abs{\hat{\theta}_{t-1}^{\top}X_{t,a} - \theta_*^{\top}X_{t,a}} &\leq \norm{\hat{\theta}_{t-1} - \theta_*}_{\bV_{t-1}} \norm{X_{t,a}}_{\bV_{t-1}^{-1}} \\
&\leq \beta_{t}(\delta)\norm{X_{t,a}}_{\bV_{t-1}^{-1}}.
\end{align*}
\end{proof}
	
\begin{algorithm}
\caption{ConComCascade}
\label{alg:ConComCascade}
\begin{algorithmic}
\STATE {//Initialization}
\STATE {Parameters: $\delta > 0, \lambda >0, 1\geq \gamma_1\geq \cdots \geq \gamma_K > 0$}
\STATE {$\hat{\theta}_0 = 0, \beta_0(\delta) = 0, \bV_0 = \lambda I$}
\STATE{}

\FORALL {$t=1,2,\ldots$}
\STATE {Obtain context $X_{t,a}$ for all $a\in E$}
\STATE{}

\STATE {//Compute UCBs}
\STATE {$\forall a\in E: $}
\STATE {$\bU_t(a) \leftarrow \min\{\hat{\theta}_{t-1}^{\top}X_{t,a} + \beta_{t-1}(\delta)\norm{X_{t,a}}_{\bV_{t-1}^{-1}}, 1\}$}
\STATE {}
		
\STATE {//Choose action $\bA_t$ using UCBs $\bU_t$}
\STATE {$\bA_t=(\ba_1^t,...,\ba_{\abs{\bA_t}}^t) \leftarrow \argmax_{A \in \cS} f(A, \bU_t)$}
\STATE {Play $\bA_t$ and observe $\bO_t, \bw_t(\ba_k^t), k\in[\bO_t]$}
\STATE {}
			
\STATE {//Update statistics}
\STATE {$\bV_{t} \leftarrow \bV_{t-1} + \sum_{k=1}^{\bO_t} \gamma_k^2 X_{t, \ba_k^t}X_{t, \ba_k^t}^{\top}$}
\STATE {$X^t \leftarrow ((X^{t-1})^\top, \gamma_1 X_{t, \ba_1^t},  ..., \gamma_{\bO_t} X_{t, \ba_{\bO_t}^t})^{\top}$}
\STATE {$Y^t \leftarrow ((Y^{t-1})^\top, \gamma_1 \bw(\ba_1^t),  ..., \gamma_{\bO_t} \bw_t(\ba_{\bO_t}^t))^{\top}$}
\STATE {$\hat{\theta}_t \leftarrow ((X^{t})^\top X^{t} + \lambda I)^{-1} (X^{t})^\top \bY^t$}
\STATE {$\beta_{t}(\delta) \leftarrow \sqrt{\log(\det(\bV_{t})) + 2 \log(\frac{1}{\delta})} + \sqrt{\lambda}$}
			
\ENDFOR {~~$t$}
\end{algorithmic}
\end{algorithm}
	
	
\section{Analysis}
Let
\begin{equation}
C_\gamma = \sum_{k=1}^{K} \gamma_k'^2.
\end{equation}
	
\begin{lemma}
\label{lem:increasing} 
If $1 \leq \gamma_1 \leq ... \leq \gamma_K \leq \gamma_0 \leq 1$ and we have $w \leq w'$ as column vectors in $\RR^L$, then for any $A\in\cS$,
$$
f(A, w) \leq f(A, w').
$$
\end{lemma}
\begin{proof}
It suffices to prove that $\cA = [m]$ where $1 \leq m \leq K$. Because for $1 \leq k \leq m$,
\begin{align*}
&~~-\gamma_k + \sum_{i=k+1}^m \gamma_i(1- w_i')\prod_{j=k+1}^{i-1}w_j' + \gamma_0\prod_{j=k+1}^{m}w_j'\\
&\geq \gamma_k (-1 + \sum_{i=k+1}^m (1- w_i')\prod_{j=k+1}^{i-1}w_j' +  \prod_{j=k+1}^{m}w_j')\\
&=\gamma_k \cdot 0 = 0,
\end{align*}
then
\begin{align*}
&~~\gamma_k (1 - w_k) + w_k\sum_{i=k+1}^m \gamma_i(1- w_i')\prod_{j=k+1}^{i-1}w_j'\\
&\qquad \qquad + \gamma_0 w_k\prod_{j=k+1}^{m}w_j'\\
& \leq \gamma_k (1 - w_k') + w_k'\sum_{i=k+1}^m \gamma_i(1- w_i')\prod_{j=k+1}^{i-1}w_j'\\
&\qquad \qquad + \gamma_0 w_k'\prod_{j=k+1}^{m}w_j'.
\end{align*}
Therefore, 
\begin{align*}
& f(A; w_1,...,w_{k-1},w_k,w_{k+1}',...,w_m')\\
&=\sum_{i=1}^{k-1} \gamma_i(1 - w_i)\prod_{j=1}^{i-1}w_j + \prod_{j=1}^{k-1}w_j [\gamma_k (1 - w_k) \\
&\qquad + w_k\sum_{i=k+1}^m \gamma_i(1- w_i')\prod_{j=k+1}^{i-1}w_j' + \gamma_0 w_k\prod_{j=k+1}^{m}w_j'\\
&\leq \sum_{i=1}^{k-1} \gamma_i(1 - w_i)\prod_{j=1}^{i-1}w_j + \prod_{j=1}^{k-1}w_j [\gamma_k (1 - w_k') \\
&\qquad + w_k'\sum_{i=k+1}^m \gamma_i(1- w_i')\prod_{j=k+1}^{i-1}w_j' + \gamma_0 w_k'\prod_{j=k+1}^{m}w_j'\\
&=f(A; w_1,...,w_{k-1},w_{k}',w_{k+1}',...,w_m').
\end{align*}
Then we obtain the result.
\end{proof}
	
\begin{lemma}
\label{lem:estimateTech}
Suppose $0 \leq p_1,...,p_K \leq 1$ and $u_1,...,u_K \geq 0$. Let $w_i = \min\{p_i + u_i, 1\}$. Then
\begin{align*}
&~~f(K, w) \leq f(K, p) + \sum_{k=1}^{K} \gamma_k' u_k
\end{align*}
\end{lemma}
\begin{proof}
We prove this by induction. It holds obviously for $K=1$.
\begin{align*}
&~~f(K+1, w)\\
&= f(K, w) -\gamma_0 \prod_{k=1}^{K} w_k + \gamma_{K+1} (1 - w_{K+1}) \prod_{k=1}^{K} w_k\\
&~~ + \gamma_0 \prod_{k=1}^{K+1} w_k\\
&= f(K, w) + (\gamma_{K+1} - \gamma_0) (1 - w_{K+1}) \prod_{k=1}^{K} w_k\\
&\leq f(K, w) + (\gamma_{K+1} - \gamma_0) (1 - w_{K+1}) \prod_{k=1}^{K} p_k\\
&\leq f(K, w) + (\gamma_{K+1} - \gamma_0) (1 - p_{K+1} - u_{K+1}) \prod_{k=1}^{K} p_k\\
&\leq f(K, w) + (\gamma_{K+1} - \gamma_0) (1 - p_{K+1}) \prod_{k=1}^{K} p_k\\
&~~ + (\gamma_0 - \gamma_{K+1}) u_{K+1}\\
&\leq f(K+1, p) + \sum_{k=1}^{K+1} \gamma_k' u_k
\end{align*}
\end{proof}
	
Let $p_A$ denote the probability that all of items in $A$ are observed. Then $p_A = \prod_{k=1}^{\abs{A}-1}w(a_i)$.

\begin{lemma}
Suppose $A = (a_1, ..., a_{\abs{A}})$. Let $B_k = (a_1, ..., a_k), k \leq \abs{A}$ be a prefix of $A$. Then we have the following properties.
\begin{itemize}
\item[(1)] $f(A, w) \leq \gamma_0 p_A + \gamma_{\abs{A}}$;
\item[(2)] $f(B_{k+1}, w) \leq f(B_k, w)$;
\item[(3)] $p_{B_{k+1}} \leq p_{B_k}$;
\item[(4)] $f(B_k, w) \leq \gamma_0 p_{B_{k+1}} + \gamma_{k+1}$.
\end{itemize}	
\end{lemma}

\begin{proof}
It suffices to prove these properties when $A = [m]$.
\begin{itemize}
\item[(1)]
$f(A, w) \leq \gamma_m(1 - p_A w_m) + \gamma_0 p_A w_m \leq \gamma_m + \gamma_0 p_A$

\item[(2)]
\begin{align*}
&f(B_k, w) - f(B_{k+1}, w)\\
&=\gamma_0 w_1 \cdots w_k - \gamma_{k+1} w_1 \cdots w_k (1 - w_{k+1})\\
&~~ - \gamma_0 w_1 \cdots w_{k} w_{k+1}\\
&=(\gamma_0 - \gamma_{k+1}) w_1 \cdots w_k (1 - w_{k+1}) \geq 0.
\end{align*}

\item[(3)]
Obvious.

\item[(4)]
$f(B_k, w) \leq \gamma_k (1 - p_{B_{k+1}}) + \gamma_0 p_{B_{k+1}}\leq \gamma_0 p_{B_{k+1}} + \gamma_{k}$
\end{itemize}
\end{proof}

\begin{proposition}
Suppose $A = (a_1, ..., a_{\abs{A}})$. Then there exists a prefix $B$ of $A$, such that 
$$
p_{B} \geq \frac{\frac{1}{2}f^* - \gamma_{B}}{\gamma_0}, \qquad \Delta_{B} \geq \frac{1}{2}\Delta_A.
$$ 
\end{proposition}
\begin{proof}
	Notice that $f^* \leq \gamma_0$. By above lemma, we have $[f(A,w), \gamma_0] \subset \bigcup [f(B_k,w), \gamma_0 p_{B_k} + \gamma_k]$.
\end{proof}


\begin{proposition}
Suppose $\gamma_K \leq \frac{1}{4} f_t^*$. With high probability $1-\delta$, 
$$
E[\Delta_{\bA_t}|\cH_t] \leq \frac{4}{f_t^*} \beta_{t-1}(\delta)\sum_{k=1}^{\bO_t}\norm{\gamma_k' X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
\end{proposition}
\begin{proof}
By above proposition, there exists a prefix $\bB_t$ of $\bA_t$ such that $p_{\bB_t} \geq \frac{1}{4}\frac{f_t^*}{\gamma_0}$ and $\Delta_{\bB_t} \geq \frac{1}{2}\Delta_A$. Then we have
$$
\EE[\Delta_{\bA_t} | \cH_{t-1}] \leq \frac{4}{f_t^*} \EE[\Delta_{\bB_t} \bOne\{\Delta_{\bB_t} > 0, \bO_t \geq \abs{\bB_t}\}
$$
Using $f(\bA_t^*,\bU_t) \leq f(\bA_t,\bU_t) \leq f(\bB_t,\bU_t)$ and lemma \ref{lem:estimateU}, \ref{lem:increasing}, it holds that
$$
f(\bA_t^*, \theta_*^{\top}X_t) \leq f(\bB_t,\bU_t).
$$
Then by lemma \ref{lem:estimateTech}
$$
f(\bB_t,\bU_t) \leq f(\bB_t, \theta_*^{\top}X_t) + \sum_{k=1}^{\abs{\bB_t}}\gamma_k'\beta_{t-1}(\delta)\norm{X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
Therefore,
$$
\Delta_{\bB_t} \leq \sum_{k=1}^{\abs{\bB_t}}\gamma_k'\beta_{t-1}(\delta)\norm{X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}.
$$
\end{proof}

\begin{corollary}
With high probability $1-\delta$, 
$$
E[\Delta_{\bA_t}|\cH_t] \leq 4\sqrt{\beta_{t-1}(\delta) \EE[\sum_{k=1}^{\bO_t}\norm{\gamma_k' X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}]}.
$$
\end{corollary}
	
\begin{theorem}
For any $\delta > 0$, with probability at least $1 - \delta$, we have
\begin{equation}
R(T) = O(\frac{d}{f^*}\log(C_\gamma T)\sqrt{KT})
\end{equation}
\end{theorem}
\begin{proof}
With probability at least $1-\delta$,
\begin{equation}
\begin{split}
&R_T =\sum_{t=1}^{T} \EE_{t}[\Delta_{\bA_t}] \\
&\leq \sum_{t=1}^{T} 4 \sqrt{\beta_{t-1}(\delta) \EE[\sum_{k=1}^{\bO_t}\norm{\gamma_k' X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}]}\\
&\leq 4\sqrt{\beta_T(\Delta)} \sum_{t=1}^{T} \sqrt{\bO_t \EE[\sum_{k=1}^{\bO_t}\norm{\gamma_k'^2 X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]}\\
&\leq 4\sqrt{\beta_T(\Delta)} \sqrt{(\sum_{t=1}^{T} \bO_t) \EE[\sum_{t=1}^{T} \sum_{k=1}^{\bO_t}\norm{\gamma_k'^2 X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2]}  \\
&\leq O(d^{3/4}\log(C_\gamma T)^{3/4}\sqrt{TK})
\end{split}
\end{equation}
\end{proof}
	
\begin{lemma} % det(V_t)
$\det(\bV_t) \leq (\lambda + C_\gamma t/d)^d.$
\end{lemma}
\begin{proof}
We have $\det(\bV_t) \leq (\trace(\bV_t)/d)^d$ and
\begin{align*}
&\trace(\bV_t)\\
& = \trace(\lambda I) + \sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s,\abs{\bA_s}\}} \gamma_k'^2 \trace(X_{s,\ba_k^s} X_{s,\ba_k^s}^{\top})\\	
& = d \lambda + \sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s,\abs{\bA_s}\}} \gamma_k'^2 \norm{X_{s,\ba_k^s}}_2^2\\
& \leq d \lambda + \sum_{s=1}^t\sum_{k=1}^{K}\gamma_k'^2=d\lambda + tC_\gamma.
\end{align*}
\end{proof}

\begin{lemma} % technical lemma for det
Let $x_i \in \RR^{d \times 1}, 1 \leq i \leq n$. Then we have
$$
\det(I + \sum_{i=1}^n x_i x_i^{\top}) \geq 1 + \sum_{i=1}^n \norm{x_i}_2^2.
$$
\end{lemma}
\begin{proof}
Denote the eigenvalues of $I + \sum_{i=1}^n x_i x_i^{\top}$ by $1+\alpha_1,...,1+\alpha_d$ with $\alpha_j \geq 0$, $1\leq j\leq d$. Then
\begin{align*}
&\det(I + \sum_{i=1}^n x_i x_i^{\top})\\
&= \prod_{j=1}^d (1 + \alpha_j)\geq 1 +\sum_{j=1}^d \alpha_j =1-d + \sum_{i=1}^d (1+\alpha_i) \\
&=1-d + \trace(I + \sum_{i=1}^n x_i x_i^{\top})= 1-d + d + \sum_{i=1}^n \norm{x_i}_2^2\\
&=1 + \sum_{i=1}^n \norm{x_i}_2^2.
\end{align*}
\end{proof}

\begin{lemma}
If $\lambda \geq C_\gamma$, it holds that
$$
\sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s, \abs{\bA_s}\}}\gamma^{2k-2}\norm{X_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \leq 2\log(\frac{\det(\bV_t)}{\det(\lambda I)}).
$$
\end{lemma}
\begin{proof}
\begin{align*}
&\det(\bV_t) = \det(\bV_{t-1} + \sum_{k=1}^{\min\{\bO_t, \abs{\bA_t}\}} \gamma^{2k-2}X_{t,\ba_k^{t}}X_{t, \ba_k^{t}}^{\top})\\
&=\det(\bV_{t-1})\\
&~~~~\det(I + \sum_{k=1}^{\min\{\bO_t, \abs{\bA_t}\}} \gamma^{2k-2}\bV_{t-1}^{-1/2}X_{t,\ba_{k}^{t}} (\bV_{t-1}^{-1/2}X_{t,\ba_{k}^{t}})^{\top})\\
&\geq \det(\bV_{t-1}) (1 + \sum_{k=1}^{\min\{\bO_t, \abs{\bA_t}\}} \norm{\gamma^{k-1}X_{t,\ba_k^t}}_{\bV_{t-1}^{-1}}^2)\\
&\geq \det(\lambda I)\prod_{s=1}^{t}(1 + \sum_{k=1}^{\min\{\bO_s, \abs{\bA_s}\}} \norm{\gamma^{k-1}X_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)
\end{align*}
Using the fact that $ 2\log(1+u) \geq u$ for any $u \in [0,1]$ and 
\begin{align*}
\norm{\gamma^{k-1}X_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2 &\leq \norm{\gamma^{k-1}X_{s,\ba_k^s}}_2^2/\lambda_{\min}(\bV_{s-1})\\
& \leq \gamma^{2k-2}/\lambda
\end{align*}
we get
\begin{align*}
&\sum_{s=1}^t \sum_{k=1}^{\min\{\bO_s, \abs{\bA_s}\}}\norm{\gamma^{k-1}X_{s,\ba_{k}^s}}_{\bV_{s-1}^{-1}}^2 \\
&\leq 2\sum_{s=1}^t\log(1 + \sum_{k=1}^{\min\{\bO_s, \abs{\bA_s}\}} \norm{X_{s,\ba_k^s}}_{\bV_{s-1}^{-1}}^2)\\
&\leq 2(\log(\det(\bV_t)) - \log(\det(\lambda I)))
\end{align*}
\end{proof}
	
	
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}
	
\bibliography{cascade_reference}
\bibliographystyle{icml2015}
	
\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
